{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal Knowledge Graph Dataset Usage\n",
        "\n",
        "This notebook demonstrates how to use the Legal Knowledge Graph Dataset for training Graph Neural Networks (GNNs) on Ukrainian court document data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The dataset processes legal documents containing:\n",
        "- **Entities**: Named entities extracted from documents (persons, organizations, etc.)\n",
        "- **Relations**: Relationships between entities (e.g., \"filed by\", \"represented by\")\n",
        "- **Legal References**: Optional legal code references for enhanced context\n",
        "\n",
        "## Data Sources\n",
        "\n",
        "The dataset can load data from:\n",
        "1. **BigQuery**: Direct connection to Google Cloud BigQuery\n",
        "2. **CSV**: Local CSV files with the same schema\n",
        "\n",
        "## Features\n",
        "\n",
        "- Automatic vocabulary building for entities, relations, and legal references\n",
        "- Feature encoding for GNN training\n",
        "- Proper batching with PyTorch Geometric compatibility\n",
        "- Configurable graph size limits\n",
        "- Legal reference integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('datasets')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LegalKnowledgeGraphDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Legal Knowledge Graph training\n",
        "    Handles triplet data from BigQuery or CSV sources\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        data_source: str,  # 'bigquery' or 'csv'\n",
        "        table_id: Optional[str] = None,\n",
        "        csv_file: Optional[str] = None,\n",
        "        max_nodes: int = 100,\n",
        "        max_edges: int = 200,\n",
        "        include_legal_references: bool = True,\n",
        "        node_features_dim: int = 128,\n",
        "        edge_features_dim: int = 64,\n",
        "        transform=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dataset\n",
        "        \n",
        "        Args:\n",
        "            data_source: 'bigquery' or 'csv'\n",
        "            table_id: BigQuery table ID (for bigquery mode)\n",
        "            csv_file: CSV file path (for csv mode)\n",
        "            max_nodes: Maximum number of nodes per graph\n",
        "            max_edges: Maximum number of edges per graph\n",
        "            include_legal_references: Whether to include legal reference features\n",
        "            node_features_dim: Dimension of node features\n",
        "            edge_features_dim: Dimension of edge features\n",
        "            transform: Optional transform to apply\n",
        "        \"\"\"\n",
        "        self.data_source = data_source\n",
        "        self.table_id = table_id\n",
        "        self.csv_file = csv_file\n",
        "        self.max_nodes = max_nodes\n",
        "        self.max_edges = max_edges\n",
        "        self.include_legal_references = include_legal_references\n",
        "        self.node_features_dim = node_features_dim\n",
        "        self.edge_features_dim = edge_features_dim\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load data\n",
        "        self.data = self._load_data()\n",
        "        \n",
        "        # Build vocabulary and feature encoders\n",
        "        self.node_vocab, self.edge_vocab, self.legal_ref_vocab = self._build_vocabularies()\n",
        "        \n",
        "        # Create feature encoders\n",
        "        self.node_encoder = self._create_node_encoder()\n",
        "        self.edge_encoder = self._create_edge_encoder()\n",
        "        self.legal_ref_encoder = self._create_legal_ref_encoder()\n",
        "        \n",
        "        logger.info(f\"Dataset initialized with {len(self.data)} samples\")\n",
        "        logger.info(f\"Node vocabulary size: {len(self.node_vocab)}\")\n",
        "        logger.info(f\"Edge vocabulary size: {len(self.edge_vocab)}\")\n",
        "        logger.info(f\"Legal reference vocabulary size: {len(self.legal_ref_vocab)}\")\n",
        "    \n",
        "    def _load_data(self) -> List[Dict]:\n",
        "        \"\"\"Load data from BigQuery or CSV\"\"\"\n",
        "        if self.data_source == 'bigquery':\n",
        "            return self._load_from_bigquery()\n",
        "        elif self.data_source == 'csv':\n",
        "            return self._load_from_csv()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported data source: {self.data_source}\")\n",
        "    \n",
        "    def _load_from_bigquery(self) -> List[Dict]:\n",
        "        \"\"\"Load data from BigQuery\"\"\"\n",
        "        try:\n",
        "            from google.cloud import bigquery\n",
        "            \n",
        "            client = bigquery.Client()\n",
        "            \n",
        "            # Query documents with triplets\n",
        "            sql = f\"\"\"\n",
        "                SELECT doc_id, text, tags, triplets, triplets_count\n",
        "                FROM `{self.table_id}`\n",
        "                WHERE triplets IS NOT NULL \n",
        "                  AND triplets_count > 0\n",
        "                  AND text IS NOT NULL\n",
        "                  AND tags IS NOT NULL\n",
        "                ORDER BY triplets_count DESC\n",
        "            \"\"\"\n",
        "            \n",
        "            job = client.query(sql)\n",
        "            results = job.result().to_dataframe()\n",
        "            \n",
        "            data = []\n",
        "            for _, row in results.iterrows():\n",
        "                try:\n",
        "                    # Parse triplets\n",
        "                    triplets = json.loads(row.triplets) if isinstance(row.triplets, str) else row.triplets\n",
        "                    \n",
        "                    # Parse entities\n",
        "                    entities = json.loads(row.tags) if isinstance(row.tags, str) else row.tags\n",
        "                    \n",
        "                    data.append({\n",
        "                        'doc_id': str(row.doc_id),\n",
        "                        'text': str(row.text),\n",
        "                        'entities': entities,\n",
        "                        'triplets': triplets,\n",
        "                        'triplets_count': int(row.triplets_count)\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error parsing row {row.doc_id}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            return data\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading from BigQuery: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def _load_from_csv(self) -> List[Dict]:\n",
        "        \"\"\"Load data from CSV file\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(self.csv_file)\n",
        "            \n",
        "            data = []\n",
        "            for _, row in df.iterrows():\n",
        "                try:\n",
        "                    # Parse triplets\n",
        "                    triplets = json.loads(row.triplets) if pd.notna(row.triplets) else []\n",
        "                    \n",
        "                    # Parse entities\n",
        "                    entities = json.loads(row.tags) if pd.notna(row.tags) else []\n",
        "                    \n",
        "                    data.append({\n",
        "                        'doc_id': str(row.get('doc_id', f\"doc_{len(data)}\")),\n",
        "                        'text': str(row.text),\n",
        "                        'entities': entities,\n",
        "                        'triplets': triplets,\n",
        "                        'triplets_count': len(triplets)\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error parsing row: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            return data\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading from CSV: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def _build_vocabularies(self) -> Tuple[Dict, Dict, Dict]:\n",
        "        \"\"\"Build vocabularies for nodes, edges, and legal references\"\"\"\n",
        "        node_vocab = defaultdict(int)\n",
        "        edge_vocab = defaultdict(int)\n",
        "        legal_ref_vocab = defaultdict(int)\n",
        "        \n",
        "        for sample in self.data:\n",
        "            # Collect nodes (entities)\n",
        "            for entity in sample['entities']:\n",
        "                node_vocab[entity['text']] += 1\n",
        "            \n",
        "            # Collect edges (relations)\n",
        "            for triplet in sample['triplets']:\n",
        "                edge_vocab[triplet['relation']] += 1\n",
        "                if self.include_legal_references and triplet.get('legal_reference'):\n",
        "                    legal_ref_vocab[triplet['legal_reference']] += 1\n",
        "        \n",
        "        # Convert to dictionaries with indices\n",
        "        node_vocab = {k: i for i, k in enumerate(node_vocab.keys())}\n",
        "        edge_vocab = {k: i for i, k in enumerate(edge_vocab.keys())}\n",
        "        legal_ref_vocab = {k: i for i, k in enumerate(legal_ref_vocab.keys())}\n",
        "        \n",
        "        return dict(node_vocab), dict(edge_vocab), dict(legal_ref_vocab)\n",
        "    \n",
        "    def _create_node_encoder(self):\n",
        "        \"\"\"Create node feature encoder\"\"\"\n",
        "        def encode_node(node_text: str) -> torch.Tensor:\n",
        "            # Simple one-hot encoding based on vocabulary\n",
        "            if node_text in self.node_vocab:\n",
        "                idx = self.node_vocab[node_text]\n",
        "                features = torch.zeros(self.node_features_dim)\n",
        "                features[idx % self.node_features_dim] = 1.0\n",
        "                return features\n",
        "            else:\n",
        "                # Unknown node - use zero vector\n",
        "                return torch.zeros(self.node_features_dim)\n",
        "        \n",
        "        return encode_node\n",
        "    \n",
        "    def _create_edge_encoder(self):\n",
        "        \"\"\"Create edge feature encoder\"\"\"\n",
        "        def encode_edge(relation: str) -> torch.Tensor:\n",
        "            # Simple one-hot encoding based on vocabulary\n",
        "            if relation in self.edge_vocab:\n",
        "                idx = self.edge_vocab[relation]\n",
        "                features = torch.zeros(self.edge_features_dim)\n",
        "                features[idx % self.edge_features_dim] = 1.0\n",
        "                return features\n",
        "            else:\n",
        "                # Unknown relation - use zero vector\n",
        "                return torch.zeros(self.edge_features_dim)\n",
        "        \n",
        "        return encode_edge\n",
        "    \n",
        "    def _create_legal_ref_encoder(self):\n",
        "        \"\"\"Create legal reference feature encoder\"\"\"\n",
        "        def encode_legal_ref(legal_ref: str) -> torch.Tensor:\n",
        "            # Simple one-hot encoding based on vocabulary\n",
        "            if legal_ref in self.legal_ref_vocab:\n",
        "                idx = self.legal_ref_vocab[legal_ref]\n",
        "                features = torch.zeros(self.edge_features_dim)  # Use same dim as edge features\n",
        "                features[idx % self.edge_features_dim] = 1.0\n",
        "                return features\n",
        "            else:\n",
        "                # Unknown legal reference - use zero vector\n",
        "                return torch.zeros(self.edge_features_dim)\n",
        "        \n",
        "        return encode_legal_ref\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Get a single graph sample\"\"\"\n",
        "        sample = self.data[idx]\n",
        "        \n",
        "        # Extract nodes from entities\n",
        "        nodes = []\n",
        "        node_mapping = {}  # Map entity text to node index\n",
        "        \n",
        "        for i, entity in enumerate(sample['entities'][:self.max_nodes]):\n",
        "            node_text = entity['text']\n",
        "            nodes.append(node_text)\n",
        "            node_mapping[node_text] = i\n",
        "        \n",
        "        # Create node features\n",
        "        node_features = torch.stack([\n",
        "            self.node_encoder(node_text) for node_text in nodes\n",
        "        ])\n",
        "        \n",
        "        # Extract edges from triplets\n",
        "        edges = []\n",
        "        edge_features = []\n",
        "        \n",
        "        for triplet in sample['triplets'][:self.max_edges]:\n",
        "            source = triplet['source']\n",
        "            target = triplet['target']\n",
        "            relation = triplet['relation']\n",
        "            \n",
        "            # Only include edges where both nodes exist\n",
        "            if source in node_mapping and target in node_mapping:\n",
        "                source_idx = node_mapping[source]\n",
        "                target_idx = node_mapping[target]\n",
        "                \n",
        "                edges.append([source_idx, target_idx])\n",
        "                \n",
        "                # Create edge features\n",
        "                edge_feat = self.edge_encoder(relation)\n",
        "                if self.include_legal_references and triplet.get('legal_reference'):\n",
        "                    legal_ref_feat = self.legal_ref_encoder(triplet['legal_reference'])\n",
        "                    edge_feat = torch.cat([edge_feat, legal_ref_feat])\n",
        "                \n",
        "                edge_features.append(edge_feat)\n",
        "        \n",
        "        if not edges:\n",
        "            # Create dummy edge if no edges exist\n",
        "            edges = [[0, 0]]\n",
        "            edge_features = [torch.zeros(self.edge_features_dim * (2 if self.include_legal_references else 1))]\n",
        "        \n",
        "        # Convert to tensors\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_features = torch.stack(edge_features)\n",
        "        \n",
        "        # Pad to max dimensions\n",
        "        node_features = self._pad_tensor(node_features, self.max_nodes, self.node_features_dim)\n",
        "        edge_features = self._pad_tensor(edge_features, self.max_edges, edge_features.size(-1))\n",
        "        \n",
        "        # Create graph data\n",
        "        graph_data = {\n",
        "            'node_features': node_features,\n",
        "            'edge_index': edge_index,\n",
        "            'edge_features': edge_features,\n",
        "            'num_nodes': len(nodes),\n",
        "            'num_edges': len(edges),\n",
        "            'doc_id': sample['doc_id'],\n",
        "            'triplets_count': sample['triplets_count']\n",
        "        }\n",
        "        \n",
        "        if self.transform:\n",
        "            graph_data = self.transform(graph_data)\n",
        "        \n",
        "        return graph_data\n",
        "    \n",
        "    def _pad_tensor(self, tensor: torch.Tensor, max_size: int, feature_dim: int) -> torch.Tensor:\n",
        "        \"\"\"Pad tensor to max_size\"\"\"\n",
        "        if tensor.size(0) < max_size:\n",
        "            padding = torch.zeros(max_size - tensor.size(0), feature_dim)\n",
        "            tensor = torch.cat([tensor, padding], dim=0)\n",
        "        elif tensor.size(0) > max_size:\n",
        "            tensor = tensor[:max_size]\n",
        "        return tensor\n",
        "    \n",
        "    def get_vocabulary_info(self) -> Dict:\n",
        "        \"\"\"Get vocabulary information for model initialization\"\"\"\n",
        "        return {\n",
        "            'node_vocab_size': len(self.node_vocab),\n",
        "            'edge_vocab_size': len(self.edge_vocab),\n",
        "            'legal_ref_vocab_size': len(self.legal_ref_vocab),\n",
        "            'node_features_dim': self.node_features_dim,\n",
        "            'edge_features_dim': self.edge_features_dim\n",
        "        }\n",
        "\n",
        "\n",
        "class LegalGraphCollate:\n",
        "    \"\"\"Custom collate function for batching graphs\"\"\"\n",
        "    \n",
        "    def __init__(self, max_nodes: int = 100, max_edges: int = 200):\n",
        "        self.max_nodes = max_nodes\n",
        "        self.max_edges = max_edges\n",
        "    \n",
        "    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate a batch of graphs\"\"\"\n",
        "        # Stack node features\n",
        "        node_features = torch.stack([item['node_features'] for item in batch])\n",
        "        \n",
        "        # Stack edge features\n",
        "        edge_features = torch.stack([item['edge_features'] for item in batch])\n",
        "        \n",
        "        # Create batch index for edge_index\n",
        "        edge_indices = []\n",
        "        node_counts = []\n",
        "        \n",
        "        for i, item in enumerate(batch):\n",
        "            edge_index = item['edge_index']\n",
        "            num_nodes = item['num_nodes']\n",
        "            \n",
        "            # Adjust edge indices for batch\n",
        "            edge_index[0] += i * self.max_nodes\n",
        "            edge_index[1] += i * self.max_nodes\n",
        "            \n",
        "            edge_indices.append(edge_index)\n",
        "            node_counts.append(num_nodes)\n",
        "        \n",
        "        # Concatenate edge indices\n",
        "        edge_index = torch.cat(edge_indices, dim=1)\n",
        "        \n",
        "        # Create batch index for nodes\n",
        "        batch_index = torch.cat([\n",
        "            torch.full((item['num_nodes'],), i, dtype=torch.long)\n",
        "            for i, item in enumerate(batch)\n",
        "        ])\n",
        "        \n",
        "        return {\n",
        "            'node_features': node_features,\n",
        "            'edge_index': edge_index,\n",
        "            'edge_features': edge_features,\n",
        "            'batch_index': batch_index,\n",
        "            'num_nodes': torch.tensor(node_counts),\n",
        "            'doc_ids': [item['doc_id'] for item in batch],\n",
        "            'triplets_counts': torch.tensor([item['triplets_count'] for item in batch])\n",
        "        }\n",
        "\n",
        "\n",
        "def create_dataloader(\n",
        "    dataset: LegalKnowledgeGraphDataset,\n",
        "    batch_size: int = 32,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 0\n",
        ") -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Create a DataLoader for the legal knowledge graph dataset\"\"\"\n",
        "    collate_fn = LegalGraphCollate(\n",
        "        max_nodes=dataset.max_nodes,\n",
        "        max_edges=dataset.max_edges\n",
        "    )\n",
        "    \n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Exploration\n",
        "\n",
        "Let's start by loading data from CSV (if available) or demonstrate the BigQuery connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have CSV data available\n",
        "csv_file = 'document_data.csv'\n",
        "if os.path.exists(csv_file):\n",
        "    print(f\"Found CSV file: {csv_file}\")\n",
        "    \n",
        "    # Load a small sample to explore the data structure\n",
        "    df_sample = pd.read_csv(csv_file, nrows=5)\n",
        "    print(\"\\nCSV Data Structure:\")\n",
        "    print(df_sample.columns.tolist())\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df_sample.head())\n",
        "    \n",
        "    # Initialize dataset from CSV\n",
        "    dataset_csv = LegalKnowledgeGraphDataset(\n",
        "        data_source='csv',\n",
        "        csv_file=csv_file,\n",
        "        max_nodes=50,\n",
        "        max_edges=100,\n",
        "        include_legal_references=True,\n",
        "        node_features_dim=128,\n",
        "        edge_features_dim=64\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nCSV Dataset loaded with {len(dataset_csv)} samples\")\n",
        "    \n",
        "else:\n",
        "    print(f\"CSV file {csv_file} not found. Will demonstrate with BigQuery setup.\")\n",
        "    dataset_csv = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. BigQuery Data Loading (Optional)\n",
        "\n",
        "If you have Google Cloud credentials set up, you can load data directly from BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export GOOGLE_APPLICATION_CREDENTIALS=\"lab-test-project-1-305710-xxxxxxxxxx.json\" \n",
        "!export GOOGLE_CLOUD_PROJECT=\"lab-test-project-1-305710\"\n",
        "# substitute xxxxxxxx with id of your key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BigQuery setup (uncomment and configure if you have GCP credentials)\n",
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'lab-test-project-1-305710-30eed237388b.json'\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'lab-test-project-1-305710'\n",
        "\n",
        "# Initialize dataset from BigQuery\n",
        "dataset_bq = LegalKnowledgeGraphDataset(\n",
        "    data_source='bigquery',\n",
        "    table_id='lab-test-project-1-305710.court_data_2022.processing_doc_links',\n",
        "    max_nodes=50,\n",
        "    max_edges=100,\n",
        "    include_legal_references=True,\n",
        "    node_features_dim=128,\n",
        "    edge_features_dim=64\n",
        ")\n",
        "\n",
        "print(f\"BigQuery Dataset loaded with {len(dataset_bq)} samples\")\n",
        "\n",
        "# # For now, we'll use the CSV dataset if available\n",
        "# dataset = dataset_csv if dataset_csv is not None else None\n",
        "\n",
        "# if dataset is None:\n",
        "#     print(\"No dataset available. Please ensure you have either CSV data or BigQuery credentials configured.\")\n",
        "#     print(\"\\nTo use BigQuery:\")\n",
        "#     print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
        "#     print(\"2. Set GOOGLE_CLOUD_PROJECT environment variable\")\n",
        "#     print(\"3. Uncomment the BigQuery code above\")\n",
        "# else:\n",
        "#     print(f\"Using dataset with {len(dataset)} samples\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Exploration\n",
        "\n",
        "Let's explore the dataset structure and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Get vocabulary information\n",
        "    vocab_info = dataset.get_vocabulary_info()\n",
        "    print(\"Vocabulary Information:\")\n",
        "    for key, value in vocab_info.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Explore a few samples\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE EXPLORATION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for i in range(min(3, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"  Document ID: {sample['doc_id']}\")\n",
        "        print(f\"  Number of nodes: {sample['num_nodes']}\")\n",
        "        print(f\"  Number of edges: {sample['num_edges']}\")\n",
        "        print(f\"  Triplets count: {sample['triplets_count']}\")\n",
        "        print(f\"  Node features shape: {sample['node_features'].shape}\")\n",
        "        print(f\"  Edge features shape: {sample['edge_features'].shape}\")\n",
        "        print(f\"  Edge index shape: {sample['edge_index'].shape}\")\n",
        "    \n",
        "    # Analyze data distribution\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    node_counts = []\n",
        "    edge_counts = []\n",
        "    triplet_counts = []\n",
        "    \n",
        "    for i in range(min(100, len(dataset))):  # Sample first 100\n",
        "        sample = dataset[i]\n",
        "        node_counts.append(sample['num_nodes'])\n",
        "        edge_counts.append(sample['num_edges'])\n",
        "        triplet_counts.append(sample['triplets_count'])\n",
        "    \n",
        "    print(f\"Node count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(node_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(node_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(node_counts)}\")\n",
        "    print(f\"  Max: {np.max(node_counts)}\")\n",
        "    \n",
        "    print(f\"\\nEdge count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(edge_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(edge_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(edge_counts)}\")\n",
        "    print(f\"  Max: {np.max(edge_counts)}\")\n",
        "    \n",
        "    print(f\"\\nTriplet count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(triplet_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(triplet_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(triplet_counts)}\")\n",
        "    print(f\"  Max: {np.max(triplet_counts)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
