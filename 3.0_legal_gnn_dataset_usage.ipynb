{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal Knowledge Graph Dataset Usage\n",
        "\n",
        "This notebook demonstrates how to use the Legal Knowledge Graph Dataset for training Graph Neural Networks (GNNs) on Ukrainian court document data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The dataset processes legal documents containing:\n",
        "- **Entities**: Named entities extracted from documents (persons, organizations, etc.)\n",
        "- **Relations**: Relationships between entities (e.g., \"filed by\", \"represented by\")\n",
        "- **Legal References**: Optional legal code references for enhanced context\n",
        "\n",
        "## Data Sources\n",
        "\n",
        "The dataset can load data from:\n",
        "1. **BigQuery**: Direct connection to Google Cloud BigQuery\n",
        "2. **CSV**: Local CSV files with the same schema\n",
        "\n",
        "## Features\n",
        "\n",
        "- Automatic vocabulary building for entities, relations, and legal references\n",
        "- Feature encoding for GNN training\n",
        "- Proper batching with PyTorch Geometric compatibility\n",
        "- Configurable graph size limits\n",
        "- Legal reference integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('datasets')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from legal_gnn_dataset import LegalKnowledgeGraphDataset, create_dataloader\n",
        "\n",
        "print(\"Dataset classes imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Exploration\n",
        "\n",
        "Let's start by loading data from CSV (if available) or demonstrate the BigQuery connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have CSV data available\n",
        "csv_file = 'document_data.csv'\n",
        "if os.path.exists(csv_file):\n",
        "    print(f\"Found CSV file: {csv_file}\")\n",
        "    \n",
        "    # Load a small sample to explore the data structure\n",
        "    df_sample = pd.read_csv(csv_file, nrows=5)\n",
        "    print(\"\\nCSV Data Structure:\")\n",
        "    print(df_sample.columns.tolist())\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df_sample.head())\n",
        "    \n",
        "    # Initialize dataset from CSV\n",
        "    dataset_csv = LegalKnowledgeGraphDataset(\n",
        "        data_source='csv',\n",
        "        csv_file=csv_file,\n",
        "        max_nodes=50,\n",
        "        max_edges=100,\n",
        "        include_legal_references=True,\n",
        "        node_features_dim=128,\n",
        "        edge_features_dim=64\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nCSV Dataset loaded with {len(dataset_csv)} samples\")\n",
        "    \n",
        "else:\n",
        "    print(f\"CSV file {csv_file} not found. Will demonstrate with BigQuery setup.\")\n",
        "    dataset_csv = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. BigQuery Data Loading (Optional)\n",
        "\n",
        "If you have Google Cloud credentials set up, you can load data directly from BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BigQuery setup (uncomment and configure if you have GCP credentials)\n",
        "# import os\n",
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'lab-test-project-1-305710-30eed237388b.json'\n",
        "# os.environ['GOOGLE_CLOUD_PROJECT'] = 'lab-test-project-1-305710'\n",
        "# \n",
        "# # Initialize dataset from BigQuery\n",
        "# dataset_bq = LegalKnowledgeGraphDataset(\n",
        "#     data_source='bigquery',\n",
        "#     table_id='lab-test-project-1-305710.court_data_2022.processing_doc_links',\n",
        "#     max_nodes=50,\n",
        "#     max_edges=100,\n",
        "#     include_legal_references=True,\n",
        "#     node_features_dim=128,\n",
        "#     edge_features_dim=64\n",
        "# )\n",
        "# \n",
        "# print(f\"BigQuery Dataset loaded with {len(dataset_bq)} samples\")\n",
        "\n",
        "# For now, we'll use the CSV dataset if available\n",
        "dataset = dataset_csv if dataset_csv is not None else None\n",
        "\n",
        "if dataset is None:\n",
        "    print(\"No dataset available. Please ensure you have either CSV data or BigQuery credentials configured.\")\n",
        "    print(\"\\nTo use BigQuery:\")\n",
        "    print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
        "    print(\"2. Set GOOGLE_CLOUD_PROJECT environment variable\")\n",
        "    print(\"3. Uncomment the BigQuery code above\")\n",
        "else:\n",
        "    print(f\"Using dataset with {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Exploration\n",
        "\n",
        "Let's explore the dataset structure and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Get vocabulary information\n",
        "    vocab_info = dataset.get_vocabulary_info()\n",
        "    print(\"Vocabulary Information:\")\n",
        "    for key, value in vocab_info.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Explore a few samples\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE EXPLORATION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for i in range(min(3, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"  Document ID: {sample['doc_id']}\")\n",
        "        print(f\"  Number of nodes: {sample['num_nodes']}\")\n",
        "        print(f\"  Number of edges: {sample['num_edges']}\")\n",
        "        print(f\"  Triplets count: {sample['triplets_count']}\")\n",
        "        print(f\"  Node features shape: {sample['node_features'].shape}\")\n",
        "        print(f\"  Edge features shape: {sample['edge_features'].shape}\")\n",
        "        print(f\"  Edge index shape: {sample['edge_index'].shape}\")\n",
        "    \n",
        "    # Analyze data distribution\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    node_counts = []\n",
        "    edge_counts = []\n",
        "    triplet_counts = []\n",
        "    \n",
        "    for i in range(min(100, len(dataset))):  # Sample first 100\n",
        "        sample = dataset[i]\n",
        "        node_counts.append(sample['num_nodes'])\n",
        "        edge_counts.append(sample['num_edges'])\n",
        "        triplet_counts.append(sample['triplets_count'])\n",
        "    \n",
        "    print(f\"Node count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(node_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(node_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(node_counts)}\")\n",
        "    print(f\"  Max: {np.max(node_counts)}\")\n",
        "    \n",
        "    print(f\"\\nEdge count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(edge_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(edge_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(edge_counts)}\")\n",
        "    print(f\"  Max: {np.max(edge_counts)}\")\n",
        "    \n",
        "    print(f\"\\nTriplet count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(triplet_counts):.2f}\")\n",
        "    print(f\"  Std: {np.std(triplet_counts):.2f}\")\n",
        "    print(f\"  Min: {np.min(triplet_counts)}\")\n",
        "    print(f\"  Max: {np.max(triplet_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Node count distribution\n",
        "    axes[0, 0].hist(node_counts, bins=20, alpha=0.7, color='skyblue')\n",
        "    axes[0, 0].set_title('Node Count Distribution')\n",
        "    axes[0, 0].set_xlabel('Number of Nodes')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # Edge count distribution\n",
        "    axes[0, 1].hist(edge_counts, bins=20, alpha=0.7, color='lightgreen')\n",
        "    axes[0, 1].set_title('Edge Count Distribution')\n",
        "    axes[0, 1].set_xlabel('Number of Edges')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    # Triplet count distribution\n",
        "    axes[1, 0].hist(triplet_counts, bins=20, alpha=0.7, color='salmon')\n",
        "    axes[1, 0].set_title('Triplet Count Distribution')\n",
        "    axes[1, 0].set_xlabel('Number of Triplets')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # Scatter plot: nodes vs edges\n",
        "    axes[1, 1].scatter(node_counts, edge_counts, alpha=0.6, color='purple')\n",
        "    axes[1, 1].set_title('Nodes vs Edges')\n",
        "    axes[1, 1].set_xlabel('Number of Nodes')\n",
        "    axes[1, 1].set_ylabel('Number of Edges')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Vocabulary size information\n",
        "    print(f\"\\nVocabulary Sizes:\")\n",
        "    print(f\"  Unique entities (nodes): {vocab_info['node_vocab_size']}\")\n",
        "    print(f\"  Unique relations (edges): {vocab_info['edge_vocab_size']}\")\n",
        "    if dataset.include_legal_references:\n",
        "        print(f\"  Unique legal references: {vocab_info['legal_ref_vocab_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Simple GNN Model Definition\n",
        "\n",
        "Let's create a simple Graph Neural Network for legal document classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleLegalGNN(nn.Module):\n",
        "    \"\"\"Simple GNN for legal document classification\"\"\"\n",
        "    \n",
        "    def __init__(self, node_features_dim, edge_features_dim, hidden_dim=64, num_classes=2):\n",
        "        super(SimpleLegalGNN, self).__init__()\n",
        "        \n",
        "        self.node_features_dim = node_features_dim\n",
        "        self.edge_features_dim = edge_features_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Node feature processing\n",
        "        self.node_encoder = nn.Linear(node_features_dim, hidden_dim)\n",
        "        \n",
        "        # Edge feature processing\n",
        "        self.edge_encoder = nn.Linear(edge_features_dim, hidden_dim)\n",
        "        \n",
        "        # Graph convolution layers\n",
        "        self.conv1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.conv2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "        # Global pooling and classification\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, node_features, edge_index, edge_features, batch_index):\n",
        "        batch_size = node_features.size(0)\n",
        "        max_nodes = node_features.size(1)\n",
        "        \n",
        "        # Process node features\n",
        "        x = self.node_encoder(node_features.view(-1, node_features.size(-1)))\n",
        "        x = x.view(batch_size, max_nodes, -1)\n",
        "        \n",
        "        # Simple message passing (simplified GNN)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        \n",
        "        # Global pooling\n",
        "        x = x.transpose(1, 2)  # [batch_size, hidden_dim, max_nodes]\n",
        "        x = self.global_pool(x)  # [batch_size, hidden_dim, 1]\n",
        "        x = x.squeeze(-1)  # [batch_size, hidden_dim]\n",
        "        \n",
        "        # Classification\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "print(\"SimpleLegalGNN model defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. DataLoader Creation and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Create DataLoader\n",
        "    batch_size = 8\n",
        "    dataloader = create_dataloader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    print(f\"DataLoader created with batch size: {batch_size}\")\n",
        "    print(f\"Number of batches: {len(dataloader)}\")\n",
        "    \n",
        "    # Test a batch\n",
        "    print(\"\\nTesting batch structure:\")\n",
        "    for batch in dataloader:\n",
        "        print(f\"Batch keys: {batch.keys()}\")\n",
        "        print(f\"Node features shape: {batch['node_features'].shape}\")\n",
        "        print(f\"Edge features shape: {batch['edge_features'].shape}\")\n",
        "        print(f\"Edge index shape: {batch['edge_index'].shape}\")\n",
        "        print(f\"Batch index shape: {batch['batch_index'].shape}\")\n",
        "        print(f\"Triplets counts: {batch['triplets_counts']}\")\n",
        "        break\n",
        "    \n",
        "    # Initialize model\n",
        "    vocab_info = dataset.get_vocabulary_info()\n",
        "    model = SimpleLegalGNN(\n",
        "        node_features_dim=vocab_info['node_features_dim'],\n",
        "        edge_features_dim=vocab_info['edge_features_dim'] * (2 if dataset.include_legal_references else 1),\n",
        "        hidden_dim=64,\n",
        "        num_classes=2\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"\\nModel initialized:\")\n",
        "    print(f\"  Node features dim: {vocab_info['node_features_dim']}\")\n",
        "    print(f\"  Edge features dim: {vocab_info['edge_features_dim'] * (2 if dataset.include_legal_references else 1)}\")\n",
        "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    print(\"\\nTraining setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    model.train()\n",
        "    \n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    \n",
        "    training_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                batch['node_features'],\n",
        "                batch['edge_index'],\n",
        "                batch['edge_features'],\n",
        "                batch['batch_index']\n",
        "            )\n",
        "            \n",
        "            # Create labels (simple binary: has triplets or not)\n",
        "            labels = (batch['triplets_counts'] > 0).long()\n",
        "            \n",
        "            # Loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            # Print progress every 5 batches\n",
        "            if batch_idx % 5 == 0:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        training_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), training_losses, 'b-o')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset is not None:\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(\"Evaluating model...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "            \n",
        "            outputs = model(\n",
        "                batch['node_features'],\n",
        "                batch['edge_index'],\n",
        "                batch['edge_features'],\n",
        "                batch['batch_index']\n",
        "            )\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            labels = (batch['triplets_counts'] > 0).long()\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            # Only evaluate first few batches for demo\n",
        "            if batch_idx >= 5:\n",
        "                break\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"  Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "    \n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['No Triplets', 'Has Triplets'],\n",
        "                yticklabels=['No Triplets', 'Has Triplets'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "    \n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_predictions, \n",
        "                              target_names=['No Triplets', 'Has Triplets']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Advanced Usage Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Different dataset configurations\n",
        "print(\"Example 1: Different Dataset Configurations\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if dataset is not None:\n",
        "    # Smaller graph size\n",
        "    dataset_small = LegalKnowledgeGraphDataset(\n",
        "        data_source='csv',\n",
        "        csv_file=csv_file,\n",
        "        max_nodes=20,\n",
        "        max_edges=40,\n",
        "        include_legal_references=False,  # Disable legal references\n",
        "        node_features_dim=64,\n",
        "        edge_features_dim=32\n",
        "    )\n",
        "    \n",
        "    print(f\"Small dataset: {len(dataset_small)} samples\")\n",
        "    print(f\"Max nodes: {dataset_small.max_nodes}\")\n",
        "    print(f\"Max edges: {dataset_small.max_edges}\")\n",
        "    print(f\"Include legal references: {dataset_small.include_legal_references}\")\n",
        "    \n",
        "    # Test a sample from small dataset\n",
        "    sample = dataset_small[0]\n",
        "    print(f\"Sample node features shape: {sample['node_features'].shape}\")\n",
        "    print(f\"Sample edge features shape: {sample['edge_features'].shape}\")\n",
        "    \n",
        "    # Larger graph size\n",
        "    dataset_large = LegalKnowledgeGraphDataset(\n",
        "        data_source='csv',\n",
        "        csv_file=csv_file,\n",
        "        max_nodes=100,\n",
        "        max_edges=200,\n",
        "        include_legal_references=True,\n",
        "        node_features_dim=256,\n",
        "        edge_features_dim=128\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nLarge dataset: {len(dataset_large)} samples\")\n",
        "    print(f\"Max nodes: {dataset_large.max_nodes}\")\n",
        "    print(f\"Max edges: {dataset_large.max_edges}\")\n",
        "    print(f\"Node features dim: {dataset_large.node_features_dim}\")\n",
        "    print(f\"Edge features dim: {dataset_large.edge_features_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Custom transform function\n",
        "print(\"\\nExample 2: Custom Transform Function\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def custom_transform(sample):\n",
        "    \"\"\"Custom transform to add noise to node features\"\"\"\n",
        "    # Add small random noise to node features\n",
        "    noise = torch.randn_like(sample['node_features']) * 0.01\n",
        "    sample['node_features'] = sample['node_features'] + noise\n",
        "    return sample\n",
        "\n",
        "if dataset is not None:\n",
        "    dataset_with_transform = LegalKnowledgeGraphDataset(\n",
        "        data_source='csv',\n",
        "        csv_file=csv_file,\n",
        "        max_nodes=50,\n",
        "        max_edges=100,\n",
        "        include_legal_references=True,\n",
        "        node_features_dim=128,\n",
        "        edge_features_dim=64,\n",
        "        transform=custom_transform\n",
        "    )\n",
        "    \n",
        "    print(f\"Dataset with custom transform: {len(dataset_with_transform)} samples\")\n",
        "    \n",
        "    # Test the transform\n",
        "    sample_original = dataset[0]\n",
        "    sample_transformed = dataset_with_transform[0]\n",
        "    \n",
        "    print(f\"Original node features norm: {torch.norm(sample_original['node_features']):.4f}\")\n",
        "    print(f\"Transformed node features norm: {torch.norm(sample_transformed['node_features']):.4f}\")\n",
        "    print(f\"Difference: {torch.norm(sample_transformed['node_features'] - sample_original['node_features']):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary and Next Steps\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Dataset Loading**: How to load data from CSV or BigQuery\n",
        "2. **Data Exploration**: Understanding the structure and statistics of legal document graphs\n",
        "3. **Model Training**: Training a simple GNN for document classification\n",
        "4. **Evaluation**: Assessing model performance with metrics and visualizations\n",
        "5. **Advanced Usage**: Different configurations and custom transforms\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Advanced GNN Architectures**: Implement more sophisticated GNN layers (GCN, GAT, GraphSAGE)\n",
        "- **Multi-task Learning**: Train on multiple objectives (entity recognition, relation extraction, document classification)\n",
        "- **Hyperparameter Tuning**: Use Optuna or similar tools for optimization\n",
        "- **Model Interpretability**: Analyze which entities and relations are most important for predictions\n",
        "- **Production Deployment**: Save and serve models for real-world applications\n",
        "\n",
        "### Key Features of the Dataset:\n",
        "\n",
        "- **Flexible Data Sources**: BigQuery and CSV support\n",
        "- **Automatic Vocabulary Building**: Handles entity and relation vocabularies\n",
        "- **Legal Reference Integration**: Optional legal code references for enhanced context\n",
        "- **Configurable Graph Sizes**: Adjustable node and edge limits\n",
        "- **PyTorch Geometric Compatibility**: Easy integration with advanced GNN libraries\n",
        "- **Proper Batching**: Handles variable-sized graphs with padding\n",
        "\n",
        "The dataset is designed to be flexible and extensible for various legal document analysis tasks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}