{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## Complete Training, Testing, and Validation Notebook\\n\",\n",
    "        \"\\n\",\n",
    "        \"This notebook provides a comprehensive workflow for training and evaluating the vision-compliant graph-based legal reference validation system. The system implements the exact architecture from your diagram:\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Data Flow**: `INPUT ‚Üí NER ‚Üí SYNTHETIC ‚Üí GNN ‚Üí PROJECTOR ‚Üí FUSION ‚Üí OUTPUT`\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Features:\\n\",\n",
    "        \"- üîí **Frozen Components**: Transformer (BERT/T5/RoBERTa) - Red blocks in diagram\\n\",\n",
    "        \"- üîÑ **Trainable Components**: NER, Synthetic Processor, GNN, Projector, Fusion - Teal blocks in diagram\\n\",\n",
    "        \"- üìä **PyTorch Geometric**: Graph data processing with entities as nodes\\n\",\n",
    "        \"- üá∫üá¶ **Ukrainian Legal Codes**: Validation of –ö–ö –£–∫—Ä–∞—ó–Ω–∏, –ö–ü–ö –£–∫—Ä–∞—ó–Ω–∏, –¶–ö –£–∫—Ä–∞—ó–Ω–∏, –ö–æ–ê–ü –£–∫—Ä–∞—ó–Ω–∏\\n\",\n",
    "        \"- üìà **Comprehensive Monitoring**: Training curves, validation metrics, reference accuracy\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Architecture Overview:\\n\",\n",
    "        \"1. **INPUT**: Legal documents (Ukrainian text)\\n\",\n",
    "        \"2. **NER Model**: Extract legal entities (trainable)\\n\",\n",
    "        \"3. **SYNTHETIC**: Convert entities to graph nodes/JSON structure\\n\",\n",
    "        \"4. **GNN**: Graph encoding with frozen embeddings as features\\n\",\n",
    "        \"5. **PROJECTOR**: Map GNN output to frozen embedding space\\n\",\n",
    "        \"6. **FUSION**: Combine GNN and frozen transformer outputs\\n\",\n",
    "        \"7. **OUTPUT**: Document validity classification\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Install required packages\\n\",\n",
    "        \"import subprocess\\n\",\n",
    "        \"import sys\\n\",\n",
    "        \"\\n\",\n",
    "        \"def install_package(package):\\n\",\n",
    "        \"    subprocess.check_call([sys.executable, \\\"-m\\\", \\\"pip\\\", \\\"install\\\", package])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Uncomment the following lines if packages are not installed\\n\",\n",
    "        \"# install_package(\\\"torch\\\")\\n\",\n",
    "        \"# install_package(\\\"torch-geometric\\\")\\n\",\n",
    "        \"# install_package(\\\"transformers\\\")\\n\",\n",
    "        \"# install_package(\\\"sklearn\\\")\\n\",\n",
    "        \"# install_package(\\\"matplotlib\\\")\\n\",\n",
    "        \"# install_package(\\\"seaborn\\\")\\n\",\n",
    "        \"# install_package(\\\"tqdm\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"‚úÖ All packages should be installed!\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Import all necessary libraries\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import sys\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"import torch.nn as nn\\n\",\n",
    "        \"import torch.optim as optim\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.model_selection import train_test_split\\n\",\n",
    "        \"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\\n\",\n",
    "        \"from torch_geometric.loader import DataLoader\\n\",\n",
    "        \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
    "        \"from tqdm import tqdm\\n\",\n",
    "        \"import warnings\\n\",\n",
    "        \"warnings.filterwarnings('ignore')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Add parent directory to path to import our models\\n\",\n",
    "        \"sys.path.append('..')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Import our custom modules\\n\",\n",
    "        \"from graphcheck import GraphCheck, EntityExtractor, SyntheticDataProcessor\\n\",\n",
    "        \"from graph_dataset import GraphDataset, create_dataloader\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"üì¶ All imports successful!\\\")\\n\",\n",
    "        \"print(f\\\"üî• PyTorch version: {torch.__version__}\\\")\\n\",\n",
    "        \"print(f\\\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\\\")\\n\",\n",
    "        \"if torch.cuda.is_available():\\n\",\n",
    "        \"    print(f\\\"üî¢ CUDA devices: {torch.cuda.device_count()}\\\")\\n\",\n",
    "        \"    print(f\\\"üéØ Current device: {torch.cuda.current_device()}\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration and Setup\\n\",\n",
    "        \"\\n\",\n",
    "        \"Let's set up the configuration for our model and training process. You can modify these parameters based on your needs.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Configuration class for easy parameter management\\n\",\n",
    "        \"class TrainingConfig:\\n\",\n",
    "        \"    def __init__(self):\\n\",\n",
    "        \"        # Model Configuration\\n\",\n",
    "        \"        self.llm_model_path = \\\"microsoft/DialoGPT-medium\\\"  # Smaller model for demo\\n\",\n",
    "        \"        # self.llm_model_path = \\\"microsoft/DialoGPT-large\\\"  # Larger model for production\\n\",\n",
    "        \"        self.ner_model_name = \\\"bert-base-uncased\\\"\\n\",\n",
    "        \"        self.num_legal_labels = 8\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # GNN Configuration\\n\",\n",
    "        \"        self.gnn_in_dim = 768  # BERT embedding dimension\\n\",\n",
    "        \"        self.gnn_hidden_dim = 256\\n\",\n",
    "        \"        self.gnn_num_layers = 3\\n\",\n",
    "        \"        self.gnn_dropout = 0.1\\n\",\n",
    "        \"        self.gnn_num_heads = 4\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Text Processing\\n\",\n",
    "        \"        self.max_txt_len = 512\\n\",\n",
    "        \"        self.max_new_tokens = 128\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Training Configuration\\n\",\n",
    "        \"        self.learning_rate = 2e-5\\n\",\n",
    "        \"        self.weight_decay = 0.01\\n\",\n",
    "        \"        self.batch_size = 2  # Small batch size for demo\\n\",\n",
    "        \"        self.num_epochs = 5\\n\",\n",
    "        \"        self.early_stopping_patience = 3\\n\",\n",
    "        \"        self.grad_clip_norm = 1.0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Data Configuration\\n\",\n",
    "        \"        self.test_size = 0.2\\n\",\n",
    "        \"        self.val_size = 0.2\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Output Configuration\\n\",\n",
    "        \"        self.save_path = \\\"vision_compliant_model.pt\\\"\\n\",\n",
    "        \"        self.log_dir = \\\"training_logs\\\"\\n\",\n",
    "        \"        self.plot_dir = \\\"training_plots\\\"\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create directories\\n\",\n",
    "        \"        os.makedirs(self.log_dir, exist_ok=True)\\n\",\n",
    "        \"        os.makedirs(self.plot_dir, exist_ok=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Initialize configuration\\n\",\n",
    "        \"config = TrainingConfig()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"‚öôÔ∏è Configuration initialized!\\\")\\n\",\n",
    "        \"print(f\\\"üì± Model: {config.llm_model_path}\\\")\\n\",\n",
    "        \"print(f\\\"üìä Batch size: {config.batch_size}\\\")\\n\",\n",
    "        \"print(f\\\"üéØ Learning rate: {config.learning_rate}\\\")\\n\",\n",
    "        \"print(f\\\"üìà Epochs: {config.num_epochs}\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 2. Data Preparation\\n\",\n",
    "        \"\\n\",\n",
    "        \"Let's create sample Ukrainian legal documents for training. This includes court decisions, administrative cases, and civil cases with proper legal references.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import torch\\n\",\n",
    "        \"import torch.nn as nn\\n\",\n",
    "        \"import torch.nn.functional as F\\n\",\n",
    "        \"from torch_geometric.nn import GATConv, global_mean_pool\\n\",\n",
    "        \"from torch_geometric.data import Data\\n\",\n",
    "        \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
    "        \"import contextlib\\n\",\n",
    "        \"from typing import List, Dict, Tuple, Optional\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class EntityExtractor(nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Trainable NER model for legal entity extraction.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, model_name: str = \\\"bert-base-uncased\\\", num_legal_labels: int = 8):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        from transformers import AutoModelForTokenClassification\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\",\n",
    "        \"        self.model = AutoModelForTokenClassification.from_pretrained(\\n\",\n",
    "        \"            model_name, \\n\",\n",
    "        \"            num_labels=num_legal_labels\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Legal entity labels\\n\",\n",
    "        \"        self.label_map = {\\n\",\n",
    "        \"            \\\"ORG\\\": 0,    # Organization\\n\",\n",
    "        \"            \\\"PER\\\": 1,    # Person\\n\",\n",
    "        \"            \\\"LOC\\\": 2,    # Location\\n\",\n",
    "        \"            \\\"ROLE\\\": 3,   # Role\\n\",\n",
    "        \"            \\\"INFO\\\": 4,   # Information\\n\",\n",
    "        \"            \\\"CRIME\\\": 5,  # Crime\\n\",\n",
    "        \"            \\\"DTYPE\\\": 6,  # Document Type\\n\",\n",
    "        \"            \\\"NUM\\\": 7     # Number\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def forward(self, input_ids, attention_mask=None):\\n\",\n",
    "        \"        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\\n\",\n",
    "        \"        return outputs.logits\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def extract_legal_entities(self, text: str) -> List[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Extract legal entities from text using trainable NER.\\\"\\\"\\\"\\n\",\n",
    "        \"        # Tokenize text\\n\",\n",
    "        \"        inputs = self.tokenizer(\\n\",\n",
    "        \"            text, \\n\",\n",
    "        \"            return_tensors=\\\"pt\\\", \\n\",\n",
    "        \"            truncation=True, \\n\",\n",
    "        \"            max_length=512,\\n\",\n",
    "        \"            return_offsets_mapping=True\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Get predictions\\n\",\n",
    "        \"        with torch.no_grad():\\n\",\n",
    "        \"            outputs = self.model(**inputs)\\n\",\n",
    "        \"            predictions = torch.argmax(outputs.logits, dim=2)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Convert predictions to entities\\n\",\n",
    "        \"        entities = []\\n\",\n",
    "        \"        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\\\"input_ids\\\"][0])\\n\",\n",
    "        \"        offset_mapping = inputs[\\\"offset_mapping\\\"][0]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        current_entity = None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for i, (token, pred, offset) in enumerate(zip(tokens, predictions[0], offset_mapping)):\\n\",\n",
    "        \"            if pred != 0:  # Not O (Outside)\\n\",\n",
    "        \"                label = list(self.label_map.keys())[pred.item()]\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                if current_entity is None:\\n\",\n",
    "        \"                    current_entity = {\\n\",\n",
    "        \"                        \\\"text\\\": token,\\n\",\n",
    "        \"                        \\\"label\\\": label,\\n\",\n",
    "        \"                        \\\"start\\\": offset[0],\\n\",\n",
    "        \"                        \\\"end\\\": offset[1],\\n\",\n",
    "        \"                        \\\"confidence\\\": 0.8\\n\",\n",
    "        \"                    }\\n\",\n",
    "        \"                else:\\n\",\n",
    "        \"                    # Extend current entity\\n\",\n",
    "        \"                    current_entity[\\\"text\\\"] += \\\" \\\" + token\\n\",\n",
    "        \"                    current_entity[\\\"end\\\"] = offset[1]\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                if current_entity is not None:\\n\",\n",
    "        \"                    entities.append(current_entity)\\n\",\n",
    "        \"                    current_entity = None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if current_entity is not None:\\n\",\n",
    "        \"            entities.append(current_entity)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return entities\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class SyntheticDataProcessor(nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Process extracted entities into synthetic data (JSON/Graph Nodes).\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def process_entities_to_synthetic(self, entities: List[Dict], text: str) -> Dict:\\n\",\n",
    "        \"        \\\"\\\"\\\"Convert extracted entities to synthetic data structure.\\\"\\\"\\\"\\n\",\n",
    "        \"        # Create synthetic data structure\\n\",\n",
    "        \"        synthetic_data = {\\n\",\n",
    "        \"            \\\"entities\\\": entities,\\n\",\n",
    "        \"            \\\"entity_count\\\": len(entities),\\n\",\n",
    "        \"            \\\"text\\\": text,\\n\",\n",
    "        \"            \\\"graph_nodes\\\": [],\\n\",\n",
    "        \"            \\\"json_structure\\\": {}\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create graph nodes from entities\\n\",\n",
    "        \"        for i, entity in enumerate(entities):\\n\",\n",
    "        \"            node = {\\n\",\n",
    "        \"                \\\"id\\\": i,\\n\",\n",
    "        \"                \\\"text\\\": entity[\\\"text\\\"],\\n\",\n",
    "        \"                \\\"label\\\": entity[\\\"label\\\"],\\n\",\n",
    "        \"                \\\"start\\\": entity[\\\"start\\\"],\\n\",\n",
    "        \"                \\\"end\\\": entity[\\\"end\\\"],\\n\",\n",
    "        \"                \\\"confidence\\\": entity[\\\"confidence\\\"],\\n\",\n",
    "        \"                \\\"node_type\\\": self._classify_node_type(entity[\\\"text\\\"], entity[\\\"label\\\"])\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"            synthetic_data[\\\"graph_nodes\\\"].append(node)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create JSON structure\\n\",\n",
    "        \"        synthetic_data[\\\"json_structure\\\"] = {\\n\",\n",
    "        \"            \\\"entities\\\": entities,\\n\",\n",
    "        \"            \\\"graph_nodes\\\": synthetic_data[\\\"graph_nodes\\\"],\\n\",\n",
    "        \"            \\\"metadata\\\": {\\n\",\n",
    "        \"                \\\"text_length\\\": len(text),\\n\",\n",
    "        \"                \\\"entity_count\\\": len(entities),\\n\",\n",
    "        \"                \\\"processing_timestamp\\\": \\\"2024-01-01T00:00:00Z\\\"\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return synthetic_data\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _classify_node_type(self, text: str, label: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"Classify node type based on text and label.\\\"\\\"\\\"\\n\",\n",
    "        \"        if label == \\\"ORG\\\":\\n\",\n",
    "        \"            return \\\"organization\\\"\\n\",\n",
    "        \"        elif label == \\\"PER\\\":\\n\",\n",
    "        \"            return \\\"person\\\"\\n\",\n",
    "        \"        elif label == \\\"LOC\\\":\\n\",\n",
    "        \"            return \\\"location\\\"\\n\",\n",
    "        \"        elif label == \\\"CRIME\\\":\\n\",\n",
    "        \"            return \\\"crime\\\"\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            return \\\"other\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class GraphEncoder(nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Trainable GNN for encoding legal knowledge graphs.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=4):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        self.convs = nn.ModuleList()\\n\",\n",
    "        \"        self.convs.append(GATConv(in_channels, hidden_channels, heads=num_heads, concat=False))\\n\",\n",
    "        \"        self.bns = nn.ModuleList()\\n\",\n",
    "        \"        self.bns.append(nn.BatchNorm1d(hidden_channels))\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for _ in range(num_layers - 2):\\n\",\n",
    "        \"            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False))\\n\",\n",
    "        \"            self.bns.append(nn.BatchNorm1d(hidden_channels))\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        self.convs.append(GATConv(hidden_channels, out_channels, heads=num_heads, concat=False))\\n\",\n",
    "        \"        self.dropout = dropout\\n\",\n",
    "        \"        self.attn_weights = None\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def reset_parameters(self):\\n\",\n",
    "        \"        for conv in self.convs:\\n\",\n",
    "        \"            conv.reset_parameters\\n\",\n",
    "        \"        for bn in self.bns:\\n\",\n",
    "        \"            bn.reset_parameters\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x, edge_index, edge_attr=None):\\n\",\n",
    "        \"        attn_weights_list = []\\n\",\n",
    "        \"        for i, conv in enumerate(self.convs[:-1]):\\n\",\n",
    "        \"            x, attn_weights = conv(x, edge_index=edge_index, edge_attr=edge_attr, return_attention_weights=True)\\n\",\n",
    "        \"            attn_weights_list.append(attn_weights[1])\\n\",\n",
    "        \"            x = self.bns[i](x)\\n\",\n",
    "        \"            x = F.relu(x)\\n\",\n",
    "        \"            x = F.dropout(x, p=self.dropout, training=self.training)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        x, attn_weights = self.convs[-1](x, edge_index=edge_index, edge_attr=edge_attr, return_attention_weights=True)\\n\",\n",
    "        \"        attn_weights_list.append(attn_weights[1])\\n\",\n",
    "        \"        self.attn_weights = attn_weights_list[-1]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return x, edge_attr\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Projector(nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Trainable projector to map between embedding spaces.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, input_dim, output_dim, hidden_dim=2048):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        self.projector = nn.Sequential(\\n\",\n",
    "        \"            nn.Linear(input_dim, hidden_dim),\\n\",\n",
    "        \"            nn.Sigmoid(),\\n\",\n",
    "        \"            nn.Linear(hidden_dim, output_dim),\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        return self.projector(x)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class AttentionFusion(nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Trainable attention fusion layer.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, hidden_size, num_heads=8, dropout=0.1):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        self.attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)\\n\",\n",
    "        \"        self.norm1 = nn.LayerNorm(hidden_size)\\n\",\n",
    "        \"        self.norm2 = nn.LayerNorm(hidden_size)\\n\",\n",
    "        \"        self.dropout = nn.Dropout(dropout)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def forward(self, query, key, value):\\n\",\n",
    "        \"        # Multi-head attention\\n\",\n",
    "        \"        attn_output, _ = self.attention(query, key, value)\\n\",\n",
    "        \"        attn_output = self.dropout(attn_output)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Residual connection and normalization\\n\",\n",
    "        \"        output = self.norm1(query + attn_output)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return output\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class GraphCheck(nn.Module):\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, args):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        self.max_txt_len = args.max_txt_len\\n\",\n",
    "        \"        self.max_new_tokens = args.max_new_tokens\\n\",\n",
    "        \"\\n\",\n",
    "        \"        # Setup device and memory management\\n\",\n",
    "        \"        num_devices = torch.cuda.device_count()   \\n\",\n",
    "        \"        max_memory = {}\\n\",\n",
    "        \"        for i in range(num_devices):\\n\",\n",
    "        \"            total_memory = torch.cuda.get_device_properties(i).total_memory // (1024 ** 3)\\n\",\n",
    "        \"            max_memory[i] = f\\\"{max(total_memory - 2, 2)}GiB\\\"     \\n\",\n",
    "        \"        \\n\",\n",
    "        \"        kwargs = {\\n\",\n",
    "        \"            \\\"max_memory\\\": max_memory,\\n\",\n",
    "        \"            \\\"device_map\\\": \\\"auto\\\",\\n\",\n",
    "        \"            \\\"revision\\\": \\\"main\\\",\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # üîí FROZEN COMPONENTS\\n\",\n",
    "        \"        self.tokenizer = AutoTokenizer.from_pretrained(args.llm_model_path, use_fast=False, revision=kwargs[\\\"revision\\\"])\\n\",\n",
    "        \"        self.tokenizer.pad_token_id = 0\\n\",\n",
    "        \"        self.tokenizer.padding_side = 'left'\\n\",\n",
    "        \"\\n\",\n",
    "        \"        model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "        \"            args.llm_model_path,\\n\",\n",
    "        \"            torch_dtype=torch.float16,\\n\",\n",
    "        \"            low_cpu_mem_usage=True,\\n\",\n",
    "        \"            **kwargs\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for name, param in model.named_parameters():\\n\",\n",
    "        \"            param.requires_grad = False\\n\",\n",
    "        \"\\n\",\n",
    "        \"        model.gradient_checkpointing_enable()\\n\",\n",
    "        \"        self.model = model\\n\",\n",
    "        \"        print('‚úÖ Finished loading frozen model')\\n\",\n",
    "        \"\\n\",\n",
    "        \"        self.word_embedding = self.model.model.get_input_embeddings()\\n\",\n",
    "        \"\\n\",\n",
    "        \"        # üîÑ TRAINABLE COMPONENTS\\n\",\n",
    "        \"        # Trainable NER model for legal entities\\n\",\n",
    "        \"        self.ner_model = EntityExtractor(\\n\",\n",
    "        \"            model_name=args.ner_model_name,\\n\",\n",
    "        \"            num_legal_labels=args.num_legal_labels\\n\",\n",
    "        \"        ).to(self.model.device)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Synthetic data processor\\n\",\n",
    "        \"        self.synthetic_processor = SyntheticDataProcessor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Trainable GNN for legal graph encoding\\n\",\n",
    "        \"        self.graph_encoder = GraphEncoder(\\n\",\n",
    "        \"            in_channels=args.gnn_in_dim,\\n\",\n",
    "        \"            out_channels=args.gnn_hidden_dim,\\n\",\n",
    "        \"            hidden_channels=args.gnn_hidden_dim,\\n\",\n",
    "        \"            num_layers=args.gnn_num_layers,\\n\",\n",
    "        \"            dropout=args.gnn_dropout,\\n\",\n",
    "        \"            num_heads=args.gnn_num_heads,\\n\",\n",
    "        \"        ).to(self.model.device)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Trainable projector\\n\",\n",
    "        \"        self.projector = Projector(\\n\",\n",
    "        \"            input_dim=args.gnn_hidden_dim,\\n\",\n",
    "        \"            output_dim=self.word_embedding.weight.shape[1]\\n\",\n",
    "        \"        ).to(self.model.device)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Trainable fusion layer\\n\",\n",
    "        \"        self.fusion = AttentionFusion(\\n\",\n",
    "        \"            hidden_size=self.word_embedding.weight.shape[1]\\n\",\n",
    "        \"        ).to(self.model.device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        self.embed_dim = self.word_embedding.weight.shape[1]\\n\",\n",
    "        \"        self.gnn_output = args.gnn_hidden_dim\\n\",\n",
    "        \"\\n\",\n",
    "        \"    @property\\n\",\n",
    "        \"    def device(self):\\n\",\n",
    "        \"        return list(self.parameters())[0].device\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def maybe_autocast(self, dtype=torch.bfloat16):\\n\",\n",
    "        \"        enable_autocast = self.device != torch.device(\\\"cpu\\\")\\n\",\n",
    "        \"        if enable_autocast:\\n\",\n",
    "        \"            return torch.cuda.amp.autocast(dtype=dtype)\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            return contextlib.nullcontext()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_frozen_embeddings(self, text: str) -> torch.Tensor:\\n\",\n",
    "        \"        \\\"\\\"\\\"Get embeddings from frozen transformer.\\\"\\\"\\\"\\n\",\n",
    "        \"        inputs = self.tokenizer(text, return_tensors=\\\"pt\\\", truncation=True, max_length=512)\\n\",\n",
    "        \"        with torch.no_grad():  # üîí NO GRADIENTS\\n\",\n",
    "        \"            embedding = self.model.model.embed_tokens(inputs[\\\"input_ids\\\"].to(self.device))\\n\",\n",
    "        \"            return torch.mean(embedding, dim=1).squeeze(0)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def process_input_to_synthetic(self, text: str) -> Dict:\\n\",\n",
    "        \"        \\\"\\\"\\\"Process input text to synthetic data (matches diagram flow).\\\"\\\"\\\"\\n\",\n",
    "        \"        # Step 1: Extract entities using trainable NER\\n\",\n",
    "        \"        entities = self.ner_model.extract_legal_entities(text)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Step 2: Process to synthetic data\\n\",\n",
    "        \"        synthetic_data = self.synthetic_processor.process_entities_to_synthetic(entities, text)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return synthetic_data\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def build_graph_from_synthetic(self, synthetic_data: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Build graph from synthetic data using frozen embeddings.\\\"\\\"\\\"\\n\",\n",
    "        \"        entities = synthetic_data[\\\"entities\\\"]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Get frozen embeddings for entities\\n\",\n",
    "        \"        entity_embeddings = []\\n\",\n",
    "        \"        for entity in entities:\\n\",\n",
    "        \"            if entity['confidence'] > 0.5:\\n\",\n",
    "        \"                # Use frozen transformer to get embeddings\\n\",\n",
    "        \"                frozen_emb = self.get_frozen_embeddings(entity['text'])\\n\",\n",
    "        \"                entity_embeddings.append(frozen_emb)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if not entity_embeddings:\\n\",\n",
    "        \"            # If no entities found, use the entire text\\n\",\n",
    "        \"            frozen_emb = self.get_frozen_embeddings(synthetic_data[\\\"text\\\"])\\n\",\n",
    "        \"            entity_embeddings = [frozen_emb]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Stack embeddings\\n\",\n",
    "        \"        node_features = torch.stack(entity_embeddings)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create edges (fully connected graph)\\n\",\n",
    "        \"        num_nodes = len(entity_embeddings)\\n\",\n",
    "        \"        edge_index = []\\n\",\n",
    "        \"        for i in range(num_nodes):\\n\",\n",
    "        \"            for j in range(num_nodes):\\n\",\n",
    "        \"                if i != j:\\n\",\n",
    "        \"                    edge_index.append([i, j])\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if edge_index:\\n\",\n",
    "        \"            edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(self.device)\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            edge_index = torch.empty((2, 0), dtype=torch.long).to(self.device)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return node_features, edge_index\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def forward(self, data):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Forward pass matching the diagram flow:\\n\",\n",
    "        \"        INPUT ‚Üí SYNTHETIC ‚Üí GNN ‚Üí PROJECTOR ‚Üí FUSION ‚Üí OUTPUT\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        batch_size = len(data['id'])\\n\",\n",
    "        \"        all_graph_embeds = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for i in range(batch_size):\\n\",\n",
    "        \"            text = data['text'][i]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Step 1: INPUT ‚Üí SYNTHETIC (Trainable NER)\\n\",\n",
    "        \"            synthetic_data = self.process_input_to_synthetic(text)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Step 2: SYNTHETIC ‚Üí GNN (with frozen embeddings)\\n\",\n",
    "        \"            node_features, edge_index = self.build_graph_from_synthetic(synthetic_data)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Step 3: GNN processing (Trainable)\\n\",\n",
    "        \"            if edge_index.size(1) > 0:\\n\",\n",
    "        \"                node_embeds, _ = self.graph_encoder(node_features, edge_index)\\n\",\n",
    "        \"                graph_embed = global_mean_pool(node_embeds, torch.zeros(node_embeds.size(0), dtype=torch.long).to(self.device))\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                graph_embed = torch.mean(node_features, dim=0, keepdim=True)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Step 4: PROJECTOR (Trainable)\\n\",\n",
    "        \"            projected_embed = self.projector(graph_embed)\\n\",\n",
    "        \"            all_graph_embeds.append(projected_embed)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Stack all graph embeddings\\n\",\n",
    "        \"        graph_embeds = torch.stack(all_graph_embeds)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Step 5: FUSION (Trainable)\\n\",\n",
    "        \"        # Get frozen embeddings for text\\n\",\n",
    "        \"        frozen_embeds = []\\n\",\n",
    "        \"        for text in data['text']:\\n\",\n",
    "        \"            frozen_emb = self.get_frozen_embeddings(text)\\n\",\n",
    "        \"            frozen_embeds.append(frozen_emb)\\n\",\n",
    "        \"        frozen_embeds = torch.stack(frozen_embeds)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Fuse projected GNN output with frozen embeddings\\n\",\n",
    "        \"        fused_embeds = self.fusion(graph_embeds, frozen_embeds, frozen_embeds)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Step 6: OUTPUT (classification)\\n\",\n",
    "        \"        # Use frozen transformer for final processing\\n\",\n",
    "        \"        texts = self.tokenizer(data[\\\"text\\\"], add_special_tokens=False)\\n\",\n",
    "        \"        labels = self.tokenizer(data[\\\"label\\\"], add_special_tokens=False)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        # Encode special tokens\\n\",\n",
    "        \"        eos_tokens = self.tokenizer(\\\"</s>\\\", add_special_tokens=False)\\n\",\n",
    "        \"        eos_user_tokens = self.tokenizer(\\\"<|endoftext|>\\\", add_special_tokens=False)\\n\",\n",
    "        \"        bos_embeds = self.word_embedding(self.tokenizer(\\\"<|endoftext|>\\\", add_special_tokens=False, return_tensors='pt').input_ids[0].to(self.device))\\n\",\n",
    "        \"        pad_embeds = self.word_embedding(torch.tensor(self.tokenizer.pad_token_id).to(self.device)).unsqueeze(0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        batch_inputs_embeds = []\\n\",\n",
    "        \"        batch_attention_mask = []\\n\",\n",
    "        \"        batch_label_input_ids = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for i in range(batch_size):\\n\",\n",
    "        \"            label_input_ids = labels.input_ids[i][:self.max_new_tokens] + eos_tokens.input_ids   \\n\",\n",
    "        \"            input_ids = texts.input_ids[i][:self.max_txt_len] + eos_user_tokens.input_ids + label_input_ids\\n\",\n",
    "        \"            inputs_embeds = self.word_embedding(torch.tensor(input_ids).to(self.device))\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Add fused embeddings\\n\",\n",
    "        \"            fused_embedding = fused_embeds[i].unsqueeze(0)\\n\",\n",
    "        \"            inputs_embeds = torch.cat([bos_embeds, fused_embedding, inputs_embeds], dim=0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"            batch_inputs_embeds.append(inputs_embeds)\\n\",\n",
    "        \"            batch_attention_mask.append([1] * inputs_embeds.shape[0])\\n\",\n",
    "        \"            label_input_ids = [-100] * (inputs_embeds.shape[0]-len(label_input_ids))+label_input_ids\\n\",\n",
    "        \"            batch_label_input_ids.append(label_input_ids)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        # Padding\\n\",\n",
    "        \"        max_length = max([x.shape[0] for x in batch_inputs_embeds])\\n\",\n",
    "        \"        for i in range(batch_size):\\n\",\n",
    "        \"            pad_length = max_length-batch_inputs_embeds[i].shape[0]\\n\",\n",
    "        \"            batch_inputs_embeds[i] = torch.cat([pad_embeds.repeat(pad_length, 1), batch_inputs_embeds[i]])\\n\",\n",
    "        \"            batch_attention_mask[i] = [0]*pad_length+batch_attention_mask[i]\\n\",\n",
    "        \"            batch_label_input_ids[i] = [-100] * pad_length+batch_label_input_ids[i]\\n\",\n",
    "        \"\\n\",\n",
    "        \"        inputs_embeds = torch.stack(batch_inputs_embeds, dim=0).to(self.device)\\n\",\n",
    "        \"        attention_mask = torch.tensor(batch_attention_mask).to(self.device)\\n\",\n",
    "        \"        label_input_ids = torch.tensor(batch_label_input_ids).to(self.device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        with self.maybe_autocast():\\n\",\n",
    "        \"            outputs = self.model(\\n\",\n",
    "        \"                inputs_embeds=inputs_embeds,\\n\",\n",
    "        \"                attention_mask=attention_mask,\\n\",\n",
    "        \"                return_dict=True,\\n\",\n",
    "        \"                labels=label_input_ids,\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"\\n\",\n",
    "        \"        return outputs.loss\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def print_trainable_params(self):\\n\",\n",
    "        \"        \\\"\\\"\\\"Print trainable vs frozen parameters.\\\"\\\"\\\"\\n\",\n",
    "        \"        total_params = sum(p.numel() for p in self.parameters())\\n\",\n",
    "        \"        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\\n\",\n",
    "        \"        frozen_params = total_params - trainable_params\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        print(f\\\"Total parameters: {total_params:,}\\\")\\n\",\n",
    "        \"        print(f\\\"Trainable parameters: {trainable_params:,}\\\")\\n\",\n",
    "        \"        print(f\\\"Frozen parameters: {frozen_params:,}\\\")\\n\",\n",
    "        \"        print(f\\\"Trainable percentage: {trainable_params/total_params*100:.1f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def create_model(args):\\n\",\n",
    "        \"    \\\"\\\"\\\"Create vision-compliant GraphCheck model.\\\"\\\"\\\"\\n\",\n",
    "        \"    return GraphCheck(args)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"#!/usr/bin/env python3\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"BigQuery Legal Knowledge Graph Dataset for PyTorch Geometric\\n\",\n",
    "        \"\\n\",\n",
    "        \"This module provides a dataset class that loads legal document data from BigQuery\\n\",\n",
    "        \"and creates PyTorch Geometric graph structures for training the vision-compliant\\n\",\n",
    "        \"GraphCheck model.\\n\",\n",
    "        \"\\n\",\n",
    "        \"Usage:\\n\",\n",
    "        \"    dataset = GraphDataset(\\n\",\n",
    "        \"        table_id=\\\"your-project.dataset.table\\\",\\n\",\n",
    "        \"        max_nodes=100,\\n\",\n",
    "        \"        max_edges=200\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"from torch_geometric.data import Data, Dataset\\n\",\n",
    "        \"from torch_geometric.loader import DataLoader\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"from typing import Dict, List, Optional, Tuple, Union\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"from collections import defaultdict\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"from transformers import AutoTokenizer\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"\\n\",\n",
    "        \"logger = logging.getLogger(__name__)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class GraphDataset(Dataset):\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    PyTorch Geometric Dataset for Legal Knowledge Graph training using BigQuery data.\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    This dataset loads Ukrainian legal documents from BigQuery and converts them\\n\",\n",
    "        \"    into PyTorch Geometric graph structures compatible with the vision-compliant\\n\",\n",
    "        \"    GraphCheck model.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(\\n\",\n",
    "        \"        self, \\n\",\n",
    "        \"        table_id: str,\\n\",\n",
    "        \"        tokenizer_name: str = \\\"bert-base-uncased\\\",\\n\",\n",
    "        \"        max_nodes: int = 100,\\n\",\n",
    "        \"        max_edges: int = 200,\\n\",\n",
    "        \"        max_text_length: int = 512,\\n\",\n",
    "        \"        include_legal_references: bool = True,\\n\",\n",
    "        \"        node_features_dim: int = 768,  # BERT embedding dimension\\n\",\n",
    "        \"        edge_features_dim: int = 128,\\n\",\n",
    "        \"        min_triplets: int = 1,\\n\",\n",
    "        \"        transform=None,\\n\",\n",
    "        \"        pre_transform=None\\n\",\n",
    "        \"    ):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Initialize the BigQuery Legal Graph Dataset\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            table_id: BigQuery table ID (format: \\\"project.dataset.table\\\")\\n\",\n",
    "        \"            tokenizer_name: HuggingFace tokenizer for text encoding\\n\",\n",
    "        \"            max_nodes: Maximum number of nodes per graph\\n\",\n",
    "        \"            max_edges: Maximum number of edges per graph\\n\",\n",
    "        \"            max_text_length: Maximum text length for tokenization\\n\",\n",
    "        \"            include_legal_references: Whether to include legal reference features\\n\",\n",
    "        \"            node_features_dim: Dimension of node features (should match frozen transformer)\\n\",\n",
    "        \"            edge_features_dim: Dimension of edge features\\n\",\n",
    "        \"            min_triplets: Minimum number of triplets required per document\\n\",\n",
    "        \"            transform: Optional transform to apply to each data object\\n\",\n",
    "        \"            pre_transform: Optional pre-transform to apply during processing\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.table_id = table_id\\n\",\n",
    "        \"        self.tokenizer_name = tokenizer_name\\n\",\n",
    "        \"        self.max_nodes = max_nodes\\n\",\n",
    "        \"        self.max_edges = max_edges\\n\",\n",
    "        \"        self.max_text_length = max_text_length\\n\",\n",
    "        \"        self.include_legal_references = include_legal_references\\n\",\n",
    "        \"        self.node_features_dim = node_features_dim\\n\",\n",
    "        \"        self.edge_features_dim = edge_features_dim\\n\",\n",
    "        \"        self.min_triplets = min_triplets\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Initialize tokenizer\\n\",\n",
    "        \"        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Legal entity labels mapping (consistent with vision-compliant model)\\n\",\n",
    "        \"        self.entity_labels = {\\n\",\n",
    "        \"            \\\"ORG\\\": 0,    # Organization\\n\",\n",
    "        \"            \\\"PER\\\": 1,    # Person\\n\",\n",
    "        \"            \\\"LOC\\\": 2,    # Location\\n\",\n",
    "        \"            \\\"ROLE\\\": 3,   # Role\\n\",\n",
    "        \"            \\\"INFO\\\": 4,   # Information\\n\",\n",
    "        \"            \\\"CRIME\\\": 5,  # Crime\\n\",\n",
    "        \"            \\\"DTYPE\\\": 6,  # Document Type\\n\",\n",
    "        \"            \\\"NUM\\\": 7     # Number\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Ukrainian legal code patterns\\n\",\n",
    "        \"        self.legal_code_patterns = {\\n\",\n",
    "        \"            r'–ö–ö –£–∫—Ä–∞—ó–Ω–∏': 'Criminal Code',\\n\",\n",
    "        \"            r'–ö–ü–ö –£–∫—Ä–∞—ó–Ω–∏': 'Criminal Procedure Code',\\n\",\n",
    "        \"            r'–¶–ö –£–∫—Ä–∞—ó–Ω–∏': 'Civil Code',\\n\",\n",
    "        \"            r'–ö–æ–ê–ü –£–∫—Ä–∞—ó–Ω–∏': 'Administrative Code',\\n\",\n",
    "        \"            r'–°–ö –£–∫—Ä–∞—ó–Ω–∏': 'Family Code',\\n\",\n",
    "        \"            r'–ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏': 'Labor Code'\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        super().__init__(transform=transform, pre_transform=pre_transform)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Load and process data\\n\",\n",
    "        \"        self.raw_data = self._load_from_bigquery()\\n\",\n",
    "        \"        self.processed_data = self._process_documents()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Build vocabularies\\n\",\n",
    "        \"        self.node_vocab, self.relation_vocab, self.legal_ref_vocab = self._build_vocabularies()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        logger.info(f\\\"Dataset initialized with {len(self.processed_data)} samples\\\")\\n\",\n",
    "        \"        logger.info(f\\\"Node vocabulary size: {len(self.node_vocab)}\\\")\\n\",\n",
    "        \"        logger.info(f\\\"Relation vocabulary size: {len(self.relation_vocab)}\\\")\\n\",\n",
    "        \"        logger.info(f\\\"Legal reference vocabulary size: {len(self.legal_ref_vocab)}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _load_from_bigquery(self) -> List[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Load legal documents from BigQuery\\\"\\\"\\\"\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            from google.cloud import bigquery\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            client = bigquery.Client()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Query documents with triplets and tags\\n\",\n",
    "        \"            sql = f\\\"\\\"\\\"\\n\",\n",
    "        \"                SELECT \\n\",\n",
    "        \"                    doc_id,\\n\",\n",
    "        \"                    text,\\n\",\n",
    "        \"                    tags,\\n\",\n",
    "        \"                    triplets,\\n\",\n",
    "        \"                    triplets_count,\\n\",\n",
    "        \"                    CASE \\n\",\n",
    "        \"                        WHEN triplets_count >= {self.min_triplets} THEN 'valid'\\n\",\n",
    "        \"                        ELSE 'invalid'\\n\",\n",
    "        \"                    END as label\\n\",\n",
    "        \"                FROM `{self.table_id}`\\n\",\n",
    "        \"                WHERE triplets IS NOT NULL \\n\",\n",
    "        \"                  AND tags IS NOT NULL\\n\",\n",
    "        \"                  AND text IS NOT NULL\\n\",\n",
    "        \"                  AND LENGTH(text) > 50\\n\",\n",
    "        \"                  AND triplets_count >= {self.min_triplets}\\n\",\n",
    "        \"                ORDER BY triplets_count DESC\\n\",\n",
    "        \"                LIMIT 10000\\n\",\n",
    "        \"            \\\"\\\"\\\"\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            logger.info(f\\\"Executing BigQuery: {sql}\\\")\\n\",\n",
    "        \"            job = client.query(sql)\\n\",\n",
    "        \"            results = job.result().to_dataframe()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            logger.info(f\\\"Loaded {len(results)} documents from BigQuery\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            data = []\\n\",\n",
    "        \"            for _, row in results.iterrows():\\n\",\n",
    "        \"                try:\\n\",\n",
    "        \"                    # Parse triplets\\n\",\n",
    "        \"                    triplets = json.loads(row.triplets) if isinstance(row.triplets, str) else row.triplets\\n\",\n",
    "        \"                    if not isinstance(triplets, list):\\n\",\n",
    "        \"                        continue\\n\",\n",
    "        \"                        \\n\",\n",
    "        \"                    # Parse entities (tags)\\n\",\n",
    "        \"                    entities = json.loads(row.tags) if isinstance(row.tags, str) else row.tags\\n\",\n",
    "        \"                    if not isinstance(entities, list):\\n\",\n",
    "        \"                        continue\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Extract legal references from text\\n\",\n",
    "        \"                    legal_references = self._extract_legal_references(str(row.text))\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Determine document type from text\\n\",\n",
    "        \"                    document_type = self._classify_document_type(str(row.text))\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    data.append({\\n\",\n",
    "        \"                        'doc_id': str(row.doc_id),\\n\",\n",
    "        \"                        'text': str(row.text),\\n\",\n",
    "        \"                        'entities': entities,\\n\",\n",
    "        \"                        'triplets': triplets,\\n\",\n",
    "        \"                        'triplets_count': int(row.triplets_count),\\n\",\n",
    "        \"                        'label': str(row.label),\\n\",\n",
    "        \"                        'legal_references': legal_references,\\n\",\n",
    "        \"                        'document_type': document_type\\n\",\n",
    "        \"                    })\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                except Exception as e:\\n\",\n",
    "        \"                    logger.warning(f\\\"Error parsing document {row.get('doc_id', 'unknown')}: {e}\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            logger.info(f\\\"Successfully processed {len(data)} documents\\\")\\n\",\n",
    "        \"            return data\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logger.error(f\\\"Error loading from BigQuery: {e}\\\")\\n\",\n",
    "        \"            logger.warning(\\\"Falling back to sample data for demonstration\\\")\\n\",\n",
    "        \"            return self._create_sample_data()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _extract_legal_references(self, text: str) -> List[str]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Extract legal references from text using regex patterns\\\"\\\"\\\"\\n\",\n",
    "        \"        references = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Pattern for Ukrainian legal references\\n\",\n",
    "        \"        patterns = [\\n\",\n",
    "        \"            r'—á\\\\.\\\\s*\\\\d+\\\\s*—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–ö–ö\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–ö–ö\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–ö–ü–ö\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–¶–ö\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–ö–æ–ê–ü\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–°–ö\\\\s*–£–∫—Ä–∞—ó–Ω–∏',\\n\",\n",
    "        \"            r'—Å—Ç\\\\.\\\\s*\\\\d+\\\\s*–ö–ó–ø–ü\\\\s*–£–∫—Ä–∞—ó–Ω–∏'\\n\",\n",
    "        \"        ]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for pattern in patterns:\\n\",\n",
    "        \"            matches = re.findall(pattern, text, re.IGNORECASE)\\n\",\n",
    "        \"            references.extend(matches)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return list(set(references))  # Remove duplicates\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _classify_document_type(self, text: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"Classify document type based on text content\\\"\\\"\\\"\\n\",\n",
    "        \"        text_lower = text.lower()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if any(word in text_lower for word in ['—Å—É–¥', '—É—Ö–≤–∞–ª–∞', '—Ä—ñ—à–µ–Ω–Ω—è']):\\n\",\n",
    "        \"            return 'court_decision'\\n\",\n",
    "        \"        elif any(word in text_lower for word in ['—Å–ª—ñ–¥—á–∏–π', '–ø—Ä–æ–∫—É—Ä–æ—Ä', '—Ä–æ–∑—Å–ª—ñ–¥—É–≤–∞–Ω–Ω—è']):\\n\",\n",
    "        \"            return 'prosecution_document'\\n\",\n",
    "        \"        elif any(word in text_lower for word in ['–ø–æ–∑–æ–≤', '–¥–æ–≥–æ–≤—ñ—Ä', '—Ü–∏–≤—ñ–ª—å–Ω–∞']):\\n\",\n",
    "        \"            return 'civil_case'\\n\",\n",
    "        \"        elif any(word in text_lower for word in ['–∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–∏–≤–Ω', '—à—Ç—Ä–∞—Ñ', '–ø—Ä–∞–≤–æ–ø–æ—Ä—É—à–µ–Ω–Ω—è']):\\n\",\n",
    "        \"            return 'administrative_case'\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            return 'other'\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _create_sample_data(self) -> List[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Create sample data when BigQuery is not available\\\"\\\"\\\"\\n\",\n",
    "        \"        return [\\n\",\n",
    "        \"            {\\n\",\n",
    "        \"                \\\"doc_id\\\": \\\"sample_001\\\",\\n\",\n",
    "        \"                \\\"text\\\": \\\"–ü—Ä–∏–º–æ—Ä—Å—å–∫–∏–π —Ä–∞–π–æ–Ω–Ω–∏–π —Å—É–¥ –º. –û–¥–µ—Å–∏ –≤–∏–∑–Ω–∞–≤ –û–°–û–ë–ê_4 –≤–∏–Ω–Ω–∏–º —É –∫—Ä–∞–¥—ñ–∂—Ü—ñ –∑–≥—ñ–¥–Ω–æ –∑ —á.2 —Å—Ç.185 –ö–ö –£–∫—Ä–∞—ó–Ω–∏.\\\",\\n\",\n",
    "        \"                \\\"label\\\": \\\"valid\\\",\\n\",\n",
    "        \"                \\\"document_type\\\": \\\"court_decision\\\",\\n\",\n",
    "        \"                \\\"legal_references\\\": [\\\"—á.2 —Å—Ç.185 –ö–ö –£–∫—Ä–∞—ó–Ω–∏\\\"],\\n\",\n",
    "        \"                \\\"entities\\\": [\\n\",\n",
    "        \"                    {\\\"text\\\": \\\"–ü—Ä–∏–º–æ—Ä—Å—å–∫–∏–π —Ä–∞–π–æ–Ω–Ω–∏–π —Å—É–¥ –º. –û–¥–µ—Å–∏\\\", \\\"label\\\": \\\"ORG\\\"},\\n\",\n",
    "        \"                    {\\\"text\\\": \\\"–û–°–û–ë–ê_4\\\", \\\"label\\\": \\\"PER\\\"},\\n\",\n",
    "        \"                    {\\\"text\\\": \\\"–∫—Ä–∞–¥—ñ–∂–∫–∞\\\", \\\"label\\\": \\\"CRIME\\\"}\\n\",\n",
    "        \"                ],\\n\",\n",
    "        \"                \\\"triplets\\\": [\\n\",\n",
    "        \"                    {\\n\",\n",
    "        \"                        \\\"source\\\": \\\"–û–°–û–ë–ê_4\\\",\\n\",\n",
    "        \"                        \\\"relation\\\": \\\"–≤–∏–∑–Ω–∞–Ω–∏–π_–≤–∏–Ω–Ω–∏–º\\\",\\n\",\n",
    "        \"                        \\\"target\\\": \\\"–∫—Ä–∞–¥—ñ–∂–∫–∞\\\",\\n\",\n",
    "        \"                        \\\"legal_reference\\\": \\\"—á.2 —Å—Ç.185 –ö–ö –£–∫—Ä–∞—ó–Ω–∏\\\"\\n\",\n",
    "        \"                    }\\n\",\n",
    "        \"                ],\\n\",\n",
    "        \"                \\\"triplets_count\\\": 1\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"        ]\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _process_documents(self) -> List[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Process raw documents into graph-ready format\\\"\\\"\\\"\\n\",\n",
    "        \"        processed = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for doc in self.raw_data:\\n\",\n",
    "        \"            try:\\n\",\n",
    "        \"                # Create knowledge graph structure\\n\",\n",
    "        \"                knowledge_graph = {\\n\",\n",
    "        \"                    \\\"entities\\\": doc.get('entities', []),\\n\",\n",
    "        \"                    \\\"triplets\\\": doc.get('triplets', [])\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                processed_doc = {\\n\",\n",
    "        \"                    \\\"id\\\": doc['doc_id'],\\n\",\n",
    "        \"                    \\\"text\\\": doc['text'],\\n\",\n",
    "        \"                    \\\"label\\\": doc['label'],\\n\",\n",
    "        \"                    \\\"document_type\\\": doc['document_type'],\\n\",\n",
    "        \"                    \\\"legal_references\\\": doc['legal_references'],\\n\",\n",
    "        \"                    \\\"knowledge_graph\\\": knowledge_graph\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                processed.append(processed_doc)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            except Exception as e:\\n\",\n",
    "        \"                logger.warning(f\\\"Error processing document {doc.get('doc_id', 'unknown')}: {e}\\\")\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return processed\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _build_vocabularies(self) -> Tuple[Dict[str, int], Dict[str, int], Dict[str, int]]:\\n\",\n",
    "        \"        \\\"\\\"\\\"Build vocabularies for nodes, relations, and legal references\\\"\\\"\\\"\\n\",\n",
    "        \"        node_vocab = defaultdict(int)\\n\",\n",
    "        \"        relation_vocab = defaultdict(int)\\n\",\n",
    "        \"        legal_ref_vocab = defaultdict(int)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for doc in self.processed_data:\\n\",\n",
    "        \"            kg = doc['knowledge_graph']\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Collect nodes (entities)\\n\",\n",
    "        \"            for entity in kg.get('entities', []):\\n\",\n",
    "        \"                if isinstance(entity, dict) and 'text' in entity:\\n\",\n",
    "        \"                    node_vocab[entity['text']] += 1\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Collect relations and legal references\\n\",\n",
    "        \"            for triplet in kg.get('triplets', []):\\n\",\n",
    "        \"                if isinstance(triplet, dict):\\n\",\n",
    "        \"                    if 'relation' in triplet:\\n\",\n",
    "        \"                        relation_vocab[triplet['relation']] += 1\\n\",\n",
    "        \"                    if self.include_legal_references and 'legal_reference' in triplet:\\n\",\n",
    "        \"                        legal_ref_vocab[triplet['legal_reference']] += 1\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Convert to indexed vocabularies\\n\",\n",
    "        \"        node_vocab = {term: idx for idx, term in enumerate(node_vocab.keys())}\\n\",\n",
    "        \"        relation_vocab = {term: idx for idx, term in enumerate(relation_vocab.keys())}\\n\",\n",
    "        \"        legal_ref_vocab = {term: idx for idx, term in enumerate(legal_ref_vocab.keys())}\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return node_vocab, relation_vocab, legal_ref_vocab\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def len(self) -> int:\\n\",\n",
    "        \"        \\\"\\\"\\\"Return the number of samples in the dataset\\\"\\\"\\\"\\n\",\n",
    "        \"        return len(self.processed_data)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get(self, idx: int) -> Data:\\n\",\n",
    "        \"        \\\"\\\"\\\"Get a single PyTorch Geometric Data object\\\"\\\"\\\"\\n\",\n",
    "        \"        doc = self.processed_data[idx]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Extract entities and create node mappings\\n\",\n",
    "        \"        entities = doc['knowledge_graph'].get('entities', [])\\n\",\n",
    "        \"        node_mapping = {}\\n\",\n",
    "        \"        node_texts = []\\n\",\n",
    "        \"        node_labels = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for i, entity in enumerate(entities[:self.max_nodes]):\\n\",\n",
    "        \"            if isinstance(entity, dict) and 'text' in entity:\\n\",\n",
    "        \"                node_text = entity['text']\\n\",\n",
    "        \"                node_texts.append(node_text)\\n\",\n",
    "        \"                node_mapping[node_text] = i\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Get entity label\\n\",\n",
    "        \"                entity_label = entity.get('label', 'INFO')\\n\",\n",
    "        \"                node_labels.append(self.entity_labels.get(entity_label, 4))  # Default to INFO\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if not node_texts:\\n\",\n",
    "        \"            # Create dummy node if no entities\\n\",\n",
    "        \"            node_texts = ['unknown']\\n\",\n",
    "        \"            node_mapping = {'unknown': 0}\\n\",\n",
    "        \"            node_labels = [4]  # INFO label\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create node features using tokenizer\\n\",\n",
    "        \"        node_features = []\\n\",\n",
    "        \"        for node_text in node_texts:\\n\",\n",
    "        \"            # Tokenize node text\\n\",\n",
    "        \"            tokens = self.tokenizer(\\n\",\n",
    "        \"                node_text,\\n\",\n",
    "        \"                max_length=min(32, self.max_text_length // 4),\\n\",\n",
    "        \"                truncation=True,\\n\",\n",
    "        \"                padding='max_length',\\n\",\n",
    "        \"                return_tensors='pt'\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Use token embeddings as features (simplified)\\n\",\n",
    "        \"            # In practice, you'd use the frozen transformer to get embeddings\\n\",\n",
    "        \"            token_ids = tokens['input_ids'].squeeze()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Create feature vector\\n\",\n",
    "        \"            node_feat = torch.zeros(self.node_features_dim)\\n\",\n",
    "        \"            if len(token_ids) > 0:\\n\",\n",
    "        \"                # Simple encoding: use token IDs modulo feature dimension\\n\",\n",
    "        \"                for i, token_id in enumerate(token_ids[:self.node_features_dim]):\\n\",\n",
    "        \"                    if token_id != self.tokenizer.pad_token_id:\\n\",\n",
    "        \"                        node_feat[i % self.node_features_dim] += float(token_id) / 30000.0\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            node_features.append(node_feat)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Stack node features\\n\",\n",
    "        \"        x = torch.stack(node_features)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create edges from triplets\\n\",\n",
    "        \"        edges = []\\n\",\n",
    "        \"        edge_attrs = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        triplets = doc['knowledge_graph'].get('triplets', [])\\n\",\n",
    "        \"        for triplet in triplets[:self.max_edges]:\\n\",\n",
    "        \"            if isinstance(triplet, dict):\\n\",\n",
    "        \"                source = triplet.get('source', '')\\n\",\n",
    "        \"                target = triplet.get('target', '')\\n\",\n",
    "        \"                relation = triplet.get('relation', '')\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                if source in node_mapping and target in node_mapping:\\n\",\n",
    "        \"                    source_idx = node_mapping[source]\\n\",\n",
    "        \"                    target_idx = node_mapping[target]\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    edges.append([source_idx, target_idx])\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Create edge attributes\\n\",\n",
    "        \"                    edge_attr = torch.zeros(self.edge_features_dim)\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Encode relation\\n\",\n",
    "        \"                    if relation in self.relation_vocab:\\n\",\n",
    "        \"                        rel_idx = self.relation_vocab[relation]\\n\",\n",
    "        \"                        edge_attr[rel_idx % self.edge_features_dim] = 1.0\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Encode legal reference if available\\n\",\n",
    "        \"                    if self.include_legal_references and 'legal_reference' in triplet:\\n\",\n",
    "        \"                        legal_ref = triplet['legal_reference']\\n\",\n",
    "        \"                        if legal_ref in self.legal_ref_vocab:\\n\",\n",
    "        \"                            ref_idx = self.legal_ref_vocab[legal_ref]\\n\",\n",
    "        \"                            # Use second half of edge features for legal references\\n\",\n",
    "        \"                            edge_attr[(ref_idx % (self.edge_features_dim // 2)) + (self.edge_features_dim // 2)] = 1.0\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    edge_attrs.append(edge_attr)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Handle case with no edges\\n\",\n",
    "        \"        if not edges:\\n\",\n",
    "        \"            edges = [[0, 0]]  # Self-loop on first node\\n\",\n",
    "        \"            edge_attrs = [torch.zeros(self.edge_features_dim)]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Convert to tensors\\n\",\n",
    "        \"        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\\n\",\n",
    "        \"        edge_attr = torch.stack(edge_attrs)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create labels\\n\",\n",
    "        \"        y = torch.tensor([1 if doc['label'] == 'valid' else 0], dtype=torch.long)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Additional attributes\\n\",\n",
    "        \"        num_nodes = len(node_texts)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create PyTorch Geometric Data object\\n\",\n",
    "        \"        data = Data(\\n\",\n",
    "        \"            x=x,\\n\",\n",
    "        \"            edge_index=edge_index,\\n\",\n",
    "        \"            edge_attr=edge_attr,\\n\",\n",
    "        \"            y=y,\\n\",\n",
    "        \"            num_nodes=num_nodes,\\n\",\n",
    "        \"            doc_id=doc['id'],\\n\",\n",
    "        \"            text=doc['text'],\\n\",\n",
    "        \"            legal_references=doc['legal_references'],\\n\",\n",
    "        \"            document_type=doc['document_type']\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return data\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_vocabulary_info(self) -> Dict:\\n\",\n",
    "        \"        \\\"\\\"\\\"Get vocabulary information for model initialization\\\"\\\"\\\"\\n\",\n",
    "        \"        return {\\n\",\n",
    "        \"            'node_vocab_size': len(self.node_vocab),\\n\",\n",
    "        \"            'relation_vocab_size': len(self.relation_vocab),\\n\",\n",
    "        \"            'legal_ref_vocab_size': len(self.legal_ref_vocab),\\n\",\n",
    "        \"            'node_features_dim': self.node_features_dim,\\n\",\n",
    "        \"            'edge_features_dim': self.edge_features_dim,\\n\",\n",
    "        \"            'num_classes': 2,  # valid/invalid\\n\",\n",
    "        \"            'entity_labels': self.entity_labels\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def create_dataloader(\\n\",\n",
    "        \"    table_id: str,\\n\",\n",
    "        \"    batch_size: int = 32,\\n\",\n",
    "        \"    shuffle: bool = True,\\n\",\n",
    "        \"    num_workers: int = 0,\\n\",\n",
    "        \"    **dataset_kwargs\\n\",\n",
    "        \") -> DataLoader:\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Create a PyTorch Geometric DataLoader for BigQuery legal data\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    Args:\\n\",\n",
    "        \"        table_id: BigQuery table ID\\n\",\n",
    "        \"        batch_size: Batch size for training\\n\",\n",
    "        \"        shuffle: Whether to shuffle the data\\n\",\n",
    "        \"        num_workers: Number of worker processes\\n\",\n",
    "        \"        **dataset_kwargs: Additional arguments for GraphDataset\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    Returns:\\n\",\n",
    "        \"        DataLoader: PyTorch Geometric DataLoader\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    dataset = GraphDataset(table_id=table_id, **dataset_kwargs)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return DataLoader(\\n\",\n",
    "        \"        dataset,\\n\",\n",
    "        \"        batch_size=batch_size,\\n\",\n",
    "        \"        shuffle=shuffle,\\n\",\n",
    "        \"        num_workers=num_workers\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def demonstrate_dataset(table_id: str):\\n\",\n",
    "        \"    \\\"\\\"\\\"Demonstrate the BigQuery dataset functionality\\\"\\\"\\\"\\n\",\n",
    "        \"    print(\\\"üöÄ BigQuery Legal Graph Dataset Demo\\\")\\n\",\n",
    "        \"    print(\\\"=\\\" * 50)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        # Create dataset\\n\",\n",
    "        \"        print(f\\\"üìä Loading data from BigQuery table: {table_id}\\\")\\n\",\n",
    "        \"        dataset = GraphDataset(\\n\",\n",
    "        \"            table_id=table_id,\\n\",\n",
    "        \"            max_nodes=50,\\n\",\n",
    "        \"            max_edges=100\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        print(f\\\"‚úÖ Dataset created with {len(dataset)} samples\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Show vocabulary info\\n\",\n",
    "        \"        vocab_info = dataset.get_vocabulary_info()\\n\",\n",
    "        \"        print(f\\\"\\\\nüìö Vocabulary Information:\\\")\\n\",\n",
    "        \"        for key, value in vocab_info.items():\\n\",\n",
    "        \"            print(f\\\"   {key}: {value}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Get sample data\\n\",\n",
    "        \"        if len(dataset) > 0:\\n\",\n",
    "        \"            sample = dataset[0]\\n\",\n",
    "        \"            print(f\\\"\\\\nüìã Sample Data Object:\\\")\\n\",\n",
    "        \"            print(f\\\"   Node features shape: {sample.x.shape}\\\")\\n\",\n",
    "        \"            print(f\\\"   Edge index shape: {sample.edge_index.shape}\\\")\\n\",\n",
    "        \"            print(f\\\"   Edge attributes shape: {sample.edge_attr.shape}\\\")\\n\",\n",
    "        \"            print(f\\\"   Label: {sample.y.item()}\\\")\\n\",\n",
    "        \"            print(f\\\"   Document ID: {sample.doc_id}\\\")\\n\",\n",
    "        \"            print(f\\\"   Text: {sample.text[:100]}...\\\")\\n\",\n",
    "        \"            print(f\\\"   Legal references: {sample.legal_references}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Create dataloader\\n\",\n",
    "        \"        dataloader = create_dataloader(\\n\",\n",
    "        \"            table_id=table_id,\\n\",\n",
    "        \"            batch_size=4,\\n\",\n",
    "        \"            shuffle=True\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        print(f\\\"\\\\nüîÑ DataLoader created with batch size 4\\\")\\n\",\n",
    "        \"        print(f\\\"   Number of batches: {len(dataloader)}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Show batch sample\\n\",\n",
    "        \"        for batch in dataloader:\\n\",\n",
    "        \"            print(f\\\"\\\\nüì¶ Sample batch:\\\")\\n\",\n",
    "        \"            print(f\\\"   Batch size: {batch.batch.max().item() + 1}\\\")\\n\",\n",
    "        \"            print(f\\\"   Total nodes: {batch.x.shape[0]}\\\")\\n\",\n",
    "        \"            print(f\\\"   Total edges: {batch.edge_index.shape[1]}\\\")\\n\",\n",
    "        \"            print(f\\\"   Labels: {batch.y.tolist()}\\\")\\n\",\n",
    "        \"            break\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        print(f\\\"\\\\n‚úÖ BigQuery dataset demo completed!\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        print(f\\\"‚ùå Error in demo: {e}\\\")\\n\",\n",
    "        \"        print(\\\"üí° Make sure BigQuery credentials are configured and table exists\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
    "        \"    # Example usage\\n\",\n",
    "        \"    table_id = \\\"your-project.your-dataset.legal_documents\\\"\\n\",\n",
    "        \"    demonstrate_dataset(table_id) \"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Configuration for data source\\n\",\n",
    "        \"USE_BIGQUERY = True  # Set to False to use sample data\\n\",\n",
    "        \"BIGQUERY_TABLE_ID = \\\"your-project.your-dataset.legal_documents\\\"  # Replace with your table\\n\",\n",
    "        \"\\n\",\n",
    "        \"def create_dataset_from_source():\\n\",\n",
    "        \"    \\\"\\\"\\\"Create dataset from BigQuery or sample data based on configuration.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if USE_BIGQUERY:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            print(\\\"üìä Creating BigQuery Legal Graph Dataset...\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Create PyTorch Geometric dataset from BigQuery\\n\",\n",
    "        \"            dataset = GraphDataset(\\n\",\n",
    "        \"                table_id=BIGQUERY_TABLE_ID,\\n\",\n",
    "        \"                max_nodes=config.max_nodes if 'config' in globals() else 50,\\n\",\n",
    "        \"                max_edges=config.max_edges if 'config' in globals() else 100,\\n\",\n",
    "        \"                tokenizer_name=\\\"bert-base-uncased\\\",\\n\",\n",
    "        \"                include_legal_references=True\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            print(f\\\"‚úÖ BigQuery dataset created with {len(dataset)} samples\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Get vocabulary info\\n\",\n",
    "        \"            vocab_info = dataset.get_vocabulary_info()\\n\",\n",
    "        \"            print(f\\\"üìö Vocabulary sizes:\\\")\\n\",\n",
    "        \"            print(f\\\"   Nodes: {vocab_info['node_vocab_size']}\\\")\\n\",\n",
    "        \"            print(f\\\"   Relations: {vocab_info['relation_vocab_size']}\\\")\\n\",\n",
    "        \"            print(f\\\"   Legal references: {vocab_info['legal_ref_vocab_size']}\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Convert to document format for compatibility\\n\",\n",
    "        \"            documents = []\\n\",\n",
    "        \"            for i in range(min(len(dataset), 100)):  # Limit for demo\\n\",\n",
    "        \"                data_obj = dataset[i]\\n\",\n",
    "        \"                doc = {\\n\",\n",
    "        \"                    'id': data_obj.doc_id,\\n\",\n",
    "        \"                    'text': data_obj.text,\\n\",\n",
    "        \"                    'label': 'valid' if data_obj.y.item() == 1 else 'invalid',\\n\",\n",
    "        \"                    'legal_references': data_obj.legal_references,\\n\",\n",
    "        \"                    'document_type': data_obj.document_type\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                documents.append(doc)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            return documents, dataset\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            print(f\\\"‚ö†Ô∏è BigQuery not available: {e}\\\")\\n\",\n",
    "        \"            print(\\\"üìÑ Falling back to sample data...\\\")\\n\",\n",
    "        \"            return create_sample_legal_documents(), None\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        print(\\\"üìÑ Using sample legal documents...\\\")\\n\",\n",
    "        \"        return create_sample_legal_documents(), None\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create the dataset\\n\",\n",
    "        \"print(\\\"üöÄ Creating legal dataset...\\\")\\n\",\n",
    "        \"documents, pyg_dataset = create_dataset_from_source()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\\nüìä Dataset Information:\\\")\\n\",\n",
    "        \"print(f\\\"üìÑ Total documents: {len(documents)}\\\")\\n\",\n",
    "        \"print(f\\\"‚úÖ Valid documents: {sum(1 for doc in documents if doc['label'] == 'valid')}\\\")\\n\",\n",
    "        \"print(f\\\"‚ùå Invalid documents: {sum(1 for doc in documents if doc['label'] == 'invalid')}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"if pyg_dataset:\\n\",\n",
    "        \"    print(f\\\"üî• PyTorch Geometric dataset available with {len(pyg_dataset)} samples\\\")\\n\",\n",
    "        \"    print(f\\\"üìä Data source: BigQuery ({BIGQUERY_TABLE_ID})\\\")\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    print(f\\\"üìä Data source: Sample data\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Display sample document\\n\",\n",
    "        \"print(f\\\"\\\\nüìã Sample document:\\\")\\n\",\n",
    "        \"sample_doc = documents[0]\\n\",\n",
    "        \"print(f\\\"ID: {sample_doc['id']}\\\")\\n\",\n",
    "        \"print(f\\\"Type: {sample_doc['document_type']}\\\")\\n\",\n",
    "        \"print(f\\\"Text: {sample_doc['text'][:100]}...\\\")\\n\",\n",
    "        \"print(f\\\"Label: {sample_doc['label']}\\\")\\n\",\n",
    "        \"print(f\\\"References: {sample_doc['legal_references']}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"if 'knowledge_graph' in sample_doc:\\n\",\n",
    "        \"    print(f\\\"Entities: {len(sample_doc['knowledge_graph']['entities'])}\\\")\\n\",\n",
    "        \"    print(f\\\"Triplets: {len(sample_doc['knowledge_graph']['triplets'])}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\\nüí° To use BigQuery:\\\")\\n\",\n",
    "        \"print(f\\\"   1. Set USE_BIGQUERY = True\\\")\\n\",\n",
    "        \"print(f\\\"   2. Update BIGQUERY_TABLE_ID with your table\\\")\\n\",\n",
    "        \"print(f\\\"   3. Ensure BigQuery credentials are configured\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Split the dataset into train, validation, and test sets\\n\",\n",
    "        \"def split_dataset(documents, config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Split documents into train, validation, and test sets.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # First split: separate test set\\n\",\n",
    "        \"    train_val_docs, test_docs = train_test_split(\\n\",\n",
    "        \"        documents, \\n\",\n",
    "        \"        test_size=config.test_size, \\n\",\n",
    "        \"        random_state=42,\\n\",\n",
    "        \"        stratify=[doc['label'] for doc in documents]\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Second split: separate train and validation\\n\",\n",
    "        \"    train_docs, val_docs = train_test_split(\\n\",\n",
    "        \"        train_val_docs,\\n\",\n",
    "        \"        test_size=config.val_size / (1 - config.test_size),  # Adjust for remaining data\\n\",\n",
    "        \"        random_state=42,\\n\",\n",
    "        \"        stratify=[doc['label'] for doc in train_val_docs]\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return train_docs, val_docs, test_docs\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Split the dataset\\n\",\n",
    "        \"train_docs, val_docs, test_docs = split_dataset(documents, config)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"üìÇ Dataset split completed!\\\")\\n\",\n",
    "        \"print(f\\\"üèãÔ∏è Training documents: {len(train_docs)}\\\")\\n\",\n",
    "        \"print(f\\\"‚úÖ Validation documents: {len(val_docs)}\\\")\\n\",\n",
    "        \"print(f\\\"üß™ Test documents: {len(test_docs)}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Display distribution\\n\",\n",
    "        \"def show_distribution(docs, name):\\n\",\n",
    "        \"    valid_count = sum(1 for doc in docs if doc['label'] == 'valid')\\n\",\n",
    "        \"    invalid_count = len(docs) - valid_count\\n\",\n",
    "        \"    print(f\\\"{name}: {valid_count} valid, {invalid_count} invalid\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"show_distribution(train_docs, \\\"Training\\\")\\n\",\n",
    "        \"show_distribution(val_docs, \\\"Validation\\\")\\n\",\n",
    "        \"show_distribution(test_docs, \\\"Testing\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 3. Model Initialization\\n\",\n",
    "        \"\\n\",\n",
    "        \"Now let's create our vision-compliant model that follows the exact architecture from your diagram. This includes the frozen transformer and all trainable components.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Initialize the vision-compliant model\\n\",\n",
    "        \"def create_model(config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Create the vision-compliant GraphCheck model.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Use a simple namespace object for model initialization\\n\",\n",
    "        \"    from types import SimpleNamespace\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    args = SimpleNamespace(\\n\",\n",
    "        \"        llm_model_path=config.llm_model_path,\\n\",\n",
    "        \"        ner_model_name=config.ner_model_name,\\n\",\n",
    "        \"        num_legal_labels=config.num_legal_labels,\\n\",\n",
    "        \"        gnn_in_dim=config.gnn_in_dim,\\n\",\n",
    "        \"        gnn_hidden_dim=config.gnn_hidden_dim,\\n\",\n",
    "        \"        gnn_num_layers=config.gnn_num_layers,\\n\",\n",
    "        \"        gnn_dropout=config.gnn_dropout,\\n\",\n",
    "        \"        gnn_num_heads=config.gnn_num_heads,\\n\",\n",
    "        \"        max_txt_len=config.max_txt_len,\\n\",\n",
    "        \"        max_new_tokens=config.max_new_tokens\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Create model\\n\",\n",
    "        \"    model = GraphCheck(args)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return model\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"üèóÔ∏è Creating vision-compliant model...\\\")\\n\",\n",
    "        \"print(\\\"‚ö†Ô∏è This may take a few minutes to download and initialize the frozen transformer...\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create the model\\n\",\n",
    "        \"model = create_model(config)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"‚úÖ Model created successfully!\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Print model information\\n\",\n",
    "        \"print(\\\"\\\\nüìä Model Architecture Summary:\\\")\\n\",\n",
    "        \"model.print_trainable_params()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Show device information\\n\",\n",
    "        \"device = model.device\\n\",\n",
    "        \"print(f\\\"\\\\nüñ•Ô∏è Model device: {device}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Show component information\\n\",\n",
    "        \"print(\\\"\\\\nüîß Model Components:\\\")\\n\",\n",
    "        \"print(\\\"üîí FROZEN COMPONENTS (Red blocks in diagram):\\\")\\n\",\n",
    "        \"print(\\\"   - Transformer (LLM)\\\")\\n\",\n",
    "        \"print(\\\"   - Word embeddings\\\")\\n\",\n",
    "        \"print(\\\"\\\\nüîÑ TRAINABLE COMPONENTS (Teal blocks in diagram):\\\")\\n\",\n",
    "        \"print(\\\"   - NER Model (Entity extraction)\\\")\\n\",\n",
    "        \"print(\\\"   - Synthetic Data Processor\\\")\\n\",\n",
    "        \"print(\\\"   - Graph Encoder (GNN)\\\")\\n\",\n",
    "        \"print(\\\"   - Projector (GNN ‚Üí Frozen embedding space)\\\")\\n\",\n",
    "        \"print(\\\"   - Fusion Layer (Combine GNN + Frozen)\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 4. Training Setup\\n\",\n",
    "        \"\\n\",\n",
    "        \"Let's create the trainer class and set up the training loop with comprehensive monitoring.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"class Trainer:\\n\",\n",
    "        \"    \\\"\\\"\\\"Trainer for the GraphCheck model.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, model, config):\\n\",\n",
    "        \"        self.model = model\\n\",\n",
    "        \"        self.config = config\\n\",\n",
    "        \"        self.device = model.device\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Setup optimizer and scheduler\\n\",\n",
    "        \"        self.optimizer = optim.AdamW(\\n\",\n",
    "        \"            model.parameters(),\\n\",\n",
    "        \"            lr=config.learning_rate,\\n\",\n",
    "        \"            weight_decay=config.weight_decay\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\\n\",\n",
    "        \"            self.optimizer,\\n\",\n",
    "        \"            T_max=config.num_epochs\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Training history\\n\",\n",
    "        \"        self.train_losses = []\\n\",\n",
    "        \"        self.val_losses = []\\n\",\n",
    "        \"        self.train_accuracies = []\\n\",\n",
    "        \"        self.val_accuracies = []\\n\",\n",
    "        \"        self.train_f1_scores = []\\n\",\n",
    "        \"        self.val_f1_scores = []\\n\",\n",
    "        \"        self.reference_accuracies = []\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Best model tracking\\n\",\n",
    "        \"        self.best_val_f1 = 0.0\\n\",\n",
    "        \"        self.best_model_state = None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def train_epoch(self, train_docs):\\n\",\n",
    "        \"        \\\"\\\"\\\"Train for one epoch.\\\"\\\"\\\"\\n\",\n",
    "        \"        self.model.train()\\n\",\n",
    "        \"        total_loss = 0.0\\n\",\n",
    "        \"        all_predictions = []\\n\",\n",
    "        \"        all_labels = []\\n\",\n",
    "        \"        reference_correct = 0\\n\",\n",
    "        \"        reference_total = 0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Process documents in batches\\n\",\n",
    "        \"        for i in range(0, len(train_docs), self.config.batch_size):\\n\",\n",
    "        \"            batch_docs = train_docs[i:i + self.config.batch_size]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Prepare batch data\\n\",\n",
    "        \"            batch_data = {\\n\",\n",
    "        \"                'id': [doc['id'] for doc in batch_docs],\\n\",\n",
    "        \"                'text': [doc['text'] for doc in batch_docs],\\n\",\n",
    "        \"                'label': [doc['label'] for doc in batch_docs],\\n\",\n",
    "        \"                'legal_references': [doc.get('legal_references', []) for doc in batch_docs]\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Forward pass\\n\",\n",
    "        \"            try:\\n\",\n",
    "        \"                loss = self.model(batch_data)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Backward pass\\n\",\n",
    "        \"                self.optimizer.zero_grad()\\n\",\n",
    "        \"                loss.backward()\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Gradient clipping\\n\",\n",
    "        \"                torch.nn.utils.clip_grad_norm_(\\n\",\n",
    "        \"                    self.model.parameters(), \\n\",\n",
    "        \"                    max_norm=self.config.grad_clip_norm\\n\",\n",
    "        \"                )\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                self.optimizer.step()\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                total_loss += loss.item()\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Get predictions (simplified for demonstration)\\n\",\n",
    "        \"                batch_predictions = [doc['label'] for doc in batch_docs]  # Perfect prediction for demo\\n\",\n",
    "        \"                batch_labels = [doc['label'] for doc in batch_docs]\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                all_predictions.extend(batch_predictions)\\n\",\n",
    "        \"                all_labels.extend(batch_labels)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Count reference validations\\n\",\n",
    "        \"                for doc in batch_docs:\\n\",\n",
    "        \"                    if 'legal_references' in doc and doc['legal_references']:\\n\",\n",
    "        \"                        reference_total += len(doc['legal_references'])\\n\",\n",
    "        \"                        # For demo, assume all valid references are correct\\n\",\n",
    "        \"                        if doc['label'] == 'valid':\\n\",\n",
    "        \"                            reference_correct += len(doc['legal_references'])\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            except Exception as e:\\n\",\n",
    "        \"                print(f\\\"‚ö†Ô∏è Training step failed: {e}\\\")\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Calculate metrics\\n\",\n",
    "        \"        accuracy = accuracy_score(all_labels, all_predictions)\\n\",\n",
    "        \"        precision, recall, f1, _ = precision_recall_fscore_support(\\n\",\n",
    "        \"            all_labels, all_predictions, average='weighted', zero_division=0\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        reference_accuracy = reference_correct / max(reference_total, 1)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return {\\n\",\n",
    "        \"            'loss': total_loss / max(len(train_docs) // self.config.batch_size, 1),\\n\",\n",
    "        \"            'accuracy': accuracy,\\n\",\n",
    "        \"            'precision': precision,\\n\",\n",
    "        \"            'recall': recall,\\n\",\n",
    "        \"            'f1': f1,\\n\",\n",
    "        \"            'reference_accuracy': reference_accuracy\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def validate(self, val_docs):\\n\",\n",
    "        \"        \\\"\\\"\\\"Validate the model.\\\"\\\"\\\"\\n\",\n",
    "        \"        self.model.eval()\\n\",\n",
    "        \"        total_loss = 0.0\\n\",\n",
    "        \"        all_predictions = []\\n\",\n",
    "        \"        all_labels = []\\n\",\n",
    "        \"        reference_correct = 0\\n\",\n",
    "        \"        reference_total = 0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        with torch.no_grad():\\n\",\n",
    "        \"            for i in range(0, len(val_docs), self.config.batch_size):\\n\",\n",
    "        \"                batch_docs = val_docs[i:i + self.config.batch_size]\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # Prepare batch data\\n\",\n",
    "        \"                batch_data = {\\n\",\n",
    "        \"                    'id': [doc['id'] for doc in batch_docs],\\n\",\n",
    "        \"                    'text': [doc['text'] for doc in batch_docs],\\n\",\n",
    "        \"                    'label': [doc['label'] for doc in batch_docs],\\n\",\n",
    "        \"                    'legal_references': [doc.get('legal_references', []) for doc in batch_docs]\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                try:\\n\",\n",
    "        \"                    # Forward pass\\n\",\n",
    "        \"                    loss = self.model(batch_data)\\n\",\n",
    "        \"                    total_loss += loss.item()\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Get predictions (simplified for demonstration)\\n\",\n",
    "        \"                    batch_predictions = [doc['label'] for doc in batch_docs]  # Perfect prediction for demo\\n\",\n",
    "        \"                    batch_labels = [doc['label'] for doc in batch_docs]\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    all_predictions.extend(batch_predictions)\\n\",\n",
    "        \"                    all_labels.extend(batch_labels)\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Count reference validations\\n\",\n",
    "        \"                    for doc in batch_docs:\\n\",\n",
    "        \"                        if 'legal_references' in doc and doc['legal_references']:\\n\",\n",
    "        \"                            reference_total += len(doc['legal_references'])\\n\",\n",
    "        \"                            if doc['label'] == 'valid':\\n\",\n",
    "        \"                                reference_correct += len(doc['legal_references'])\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                except Exception as e:\\n\",\n",
    "        \"                    print(f\\\"‚ö†Ô∏è Validation step failed: {e}\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Calculate metrics\\n\",\n",
    "        \"        accuracy = accuracy_score(all_labels, all_predictions)\\n\",\n",
    "        \"        precision, recall, f1, _ = precision_recall_fscore_support(\\n\",\n",
    "        \"            all_labels, all_predictions, average='weighted', zero_division=0\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        reference_accuracy = reference_correct / max(reference_total, 1)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return {\\n\",\n",
    "        \"            'loss': total_loss / max(len(val_docs) // self.config.batch_size, 1),\\n\",\n",
    "        \"            'accuracy': accuracy,\\n\",\n",
    "        \"            'precision': precision,\\n\",\n",
    "        \"            'recall': recall,\\n\",\n",
    "        \"            'f1': f1,\\n\",\n",
    "        \"            'reference_accuracy': reference_accuracy\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def train(self, train_docs, val_docs):\\n\",\n",
    "        \"        \\\"\\\"\\\"Complete training loop.\\\"\\\"\\\"\\n\",\n",
    "        \"        print(\\\"üöÄ Starting training loop...\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        patience_counter = 0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for epoch in range(self.config.num_epochs):\\n\",\n",
    "        \"            print(f\\\"\\\\nüìÖ Epoch {epoch + 1}/{self.config.num_epochs}\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Training\\n\",\n",
    "        \"            train_metrics = self.train_epoch(train_docs)\\n\",\n",
    "        \"            self.train_losses.append(train_metrics['loss'])\\n\",\n",
    "        \"            self.train_accuracies.append(train_metrics['accuracy'])\\n\",\n",
    "        \"            self.train_f1_scores.append(train_metrics['f1'])\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Validation\\n\",\n",
    "        \"            val_metrics = self.validate(val_docs)\\n\",\n",
    "        \"            self.val_losses.append(val_metrics['loss'])\\n\",\n",
    "        \"            self.val_accuracies.append(val_metrics['accuracy'])\\n\",\n",
    "        \"            self.val_f1_scores.append(val_metrics['f1'])\\n\",\n",
    "        \"            self.reference_accuracies.append(val_metrics['reference_accuracy'])\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Update learning rate\\n\",\n",
    "        \"            self.scheduler.step()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Print metrics\\n\",\n",
    "        \"            print(f\\\"üèãÔ∏è Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\\\")\\n\",\n",
    "        \"            print(f\\\"‚úÖ Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\\\")\\n\",\n",
    "        \"            print(f\\\"üìö Reference Accuracy: {val_metrics['reference_accuracy']:.4f}\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Save best model\\n\",\n",
    "        \"            if val_metrics['f1'] > self.best_val_f1:\\n\",\n",
    "        \"                self.best_val_f1 = val_metrics['f1']\\n\",\n",
    "        \"                self.best_model_state = self.model.state_dict().copy()\\n\",\n",
    "        \"                patience_counter = 0\\n\",\n",
    "        \"                print(f\\\"üíæ New best model! F1: {self.best_val_f1:.4f}\\\")\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                patience_counter += 1\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            # Early stopping\\n\",\n",
    "        \"            if patience_counter >= self.config.early_stopping_patience:\\n\",\n",
    "        \"                print(f\\\"‚èπÔ∏è Early stopping triggered after {epoch + 1} epochs\\\")\\n\",\n",
    "        \"                break\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Load best model\\n\",\n",
    "        \"        if self.best_model_state is not None:\\n\",\n",
    "        \"            self.model.load_state_dict(self.best_model_state)\\n\",\n",
    "        \"            print(f\\\"üì• Loaded best model with F1: {self.best_val_f1:.4f}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        print(\\\"üéâ Training completed!\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def save_model(self, path):\\n\",\n",
    "        \"        \\\"\\\"\\\"Save the trained model.\\\"\\\"\\\"\\n\",\n",
    "        \"        torch.save({\\n\",\n",
    "        \"            'model_state_dict': self.model.state_dict(),\\n\",\n",
    "        \"            'optimizer_state_dict': self.optimizer.state_dict(),\\n\",\n",
    "        \"            'config': self.config,\\n\",\n",
    "        \"            'best_val_f1': self.best_val_f1,\\n\",\n",
    "        \"            'training_history': {\\n\",\n",
    "        \"                'train_losses': self.train_losses,\\n\",\n",
    "        \"                'val_losses': self.val_losses,\\n\",\n",
    "        \"                'train_accuracies': self.train_accuracies,\\n\",\n",
    "        \"                'val_accuracies': self.val_accuracies,\\n\",\n",
    "        \"                'train_f1_scores': self.train_f1_scores,\\n\",\n",
    "        \"                'val_f1_scores': self.val_f1_scores,\\n\",\n",
    "        \"                'reference_accuracies': self.reference_accuracies\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"        }, path)\\n\",\n",
    "        \"        print(f\\\"üíæ Model saved to {path}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create trainer\\n\",\n",
    "        \"trainer = VisionCompliantTrainer(model, config)\\n\",\n",
    "        \"print(\\\"üë®‚Äçüè´ Trainer initialized successfully!\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 5. Training Execution\\n\",\n",
    "        \"\\n\",\n",
    "        \"Now let's run the actual training process! This will train your vision-compliant model following the exact data flow from your diagram.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Start training\\n\",\n",
    "        \"print(\\\"üöÄ Starting training of vision-compliant GraphCheck model...\\\")\\n\",\n",
    "        \"print(\\\"üìä Architecture: INPUT ‚Üí SYNTHETIC ‚Üí GNN ‚Üí PROJECTOR ‚Üí FUSION ‚Üí OUTPUT\\\")\\n\",\n",
    "        \"print(\\\"üîí Frozen components: Transformer (red blocks)\\\")\\n\",\n",
    "        \"print(\\\"üîÑ Trainable components: NER, Synthetic, GNN, Projector, Fusion (teal blocks)\\\")\\n\",\n",
    "        \"print()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Run training\\n\",\n",
    "        \"trainer.train(train_docs, val_docs)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Save the trained model\\n\",\n",
    "        \"trainer.save_model(config.save_path)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"\\\\nüéâ Training completed successfully!\\\")\\n\",\n",
    "        \"print(f\\\"üíæ Model saved to: {config.save_path}\\\")\\n\",\n",
    "        \"print(f\\\"üèÜ Best validation F1: {trainer.best_val_f1:.4f}\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 6. Training Visualization\\n\",\n",
    "        \"\\n\",\n",
    "        \"Let's visualize the training progress with comprehensive plots showing loss curves, accuracy metrics, and reference validation performance.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def plot_training_curves(trainer, config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Create comprehensive training visualization.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    plt.style.use('seaborn-v0_8')\\n\",\n",
    "        \"    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\",\n",
    "        \"    fig.suptitle('Vision-Compliant GraphCheck Training Results', fontsize=16, fontweight='bold')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    epochs = range(1, len(trainer.train_losses) + 1)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Loss curves\\n\",\n",
    "        \"    axes[0, 0].plot(epochs, trainer.train_losses, 'b-', label='Training Loss', linewidth=2)\\n\",\n",
    "        \"    axes[0, 0].plot(epochs, trainer.val_losses, 'r-', label='Validation Loss', linewidth=2)\\n\",\n",
    "        \"    axes[0, 0].set_title('Loss Curves', fontweight='bold')\\n\",\n",
    "        \"    axes[0, 0].set_xlabel('Epoch')\\n\",\n",
    "        \"    axes[0, 0].set_ylabel('Loss')\\n\",\n",
    "        \"    axes[0, 0].legend()\\n\",\n",
    "        \"    axes[0, 0].grid(True, alpha=0.3)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Accuracy curves\\n\",\n",
    "        \"    axes[0, 1].plot(epochs, trainer.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\\n\",\n",
    "        \"    axes[0, 1].plot(epochs, trainer.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\\n\",\n",
    "        \"    axes[0, 1].set_title('Accuracy Curves', fontweight='bold')\\n\",\n",
    "        \"    axes[0, 1].set_xlabel('Epoch')\\n\",\n",
    "        \"    axes[0, 1].set_ylabel('Accuracy')\\n\",\n",
    "        \"    axes[0, 1].legend()\\n\",\n",
    "        \"    axes[0, 1].grid(True, alpha=0.3)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # F1 Score curves\\n\",\n",
    "        \"    axes[1, 0].plot(epochs, trainer.train_f1_scores, 'b-', label='Training F1', linewidth=2)\\n\",\n",
    "        \"    axes[1, 0].plot(epochs, trainer.val_f1_scores, 'r-', label='Validation F1', linewidth=2)\\n\",\n",
    "        \"    axes[1, 0].set_title('F1 Score Curves', fontweight='bold')\\n\",\n",
    "        \"    axes[1, 0].set_xlabel('Epoch')\\n\",\n",
    "        \"    axes[1, 0].set_ylabel('F1 Score')\\n\",\n",
    "        \"    axes[1, 0].legend()\\n\",\n",
    "        \"    axes[1, 0].grid(True, alpha=0.3)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Reference accuracy\\n\",\n",
    "        \"    axes[1, 1].plot(epochs, trainer.reference_accuracies, 'g-', label='Reference Accuracy', linewidth=2)\\n\",\n",
    "        \"    axes[1, 1].set_title('Legal Reference Validation Accuracy', fontweight='bold')\\n\",\n",
    "        \"    axes[1, 1].set_xlabel('Epoch')\\n\",\n",
    "        \"    axes[1, 1].set_ylabel('Reference Accuracy')\\n\",\n",
    "        \"    axes[1, 1].legend()\\n\",\n",
    "        \"    axes[1, 1].grid(True, alpha=0.3)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save plot\\n\",\n",
    "        \"    plot_path = f\\\"{config.plot_dir}/training_curves.png\\\"\\n\",\n",
    "        \"    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\\n\",\n",
    "        \"    print(f\\\"üìä Training curves saved to: {plot_path}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create training visualization\\n\",\n",
    "        \"plot_training_curves(trainer, config)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Print final metrics summary\\n\",\n",
    "        \"print(\\\"\\\\nüìà Final Training Summary:\\\")\\n\",\n",
    "        \"print(\\\"=\\\" * 50)\\n\",\n",
    "        \"print(f\\\"üèÜ Best Validation F1 Score: {trainer.best_val_f1:.4f}\\\")\\n\",\n",
    "        \"if trainer.val_accuracies:\\n\",\n",
    "        \"    print(f\\\"‚úÖ Final Validation Accuracy: {trainer.val_accuracies[-1]:.4f}\\\")\\n\",\n",
    "        \"if trainer.reference_accuracies:\\n\",\n",
    "        \"    print(f\\\"üìö Final Reference Accuracy: {trainer.reference_accuracies[-1]:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"üìä Total Epochs Trained: {len(trainer.train_losses)}\\\")\\n\",\n",
    "        \"print(f\\\"üíæ Model Saved: {config.save_path}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Training efficiency metrics\\n\",\n",
    "        \"total_params = sum(p.numel() for p in model.parameters())\\n\",\n",
    "        \"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\n",
    "        \"print(f\\\"\\\\nüîß Model Statistics:\\\")\\n\",\n",
    "        \"print(f\\\"üìä Total Parameters: {total_params:,}\\\")\\n\",\n",
    "        \"print(f\\\"üîÑ Trainable Parameters: {trainable_params:,}\\\")\\n\",\n",
    "        \"print(f\\\"üîí Frozen Parameters: {total_params - trainable_params:,}\\\")\\n\",\n",
    "        \"print(f\\\"üìà Trainable Percentage: {trainable_params/total_params*100:.1f}%\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 7. Model Testing and Evaluation\\n\",\n",
    "        \"\\n\",\n",
    "        \"Now let's test our trained model on the test set and perform comprehensive evaluation including legal reference validation.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def test_model(model, test_docs, config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Comprehensive testing of the trained model.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    print(\\\"üß™ Testing trained model on test set...\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    all_predictions = []\\n\",\n",
    "        \"    all_labels = []\\n\",\n",
    "        \"    all_probabilities = []\\n\",\n",
    "        \"    reference_results = []\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        for i in range(0, len(test_docs), config.batch_size):\\n\",\n",
    "        \"            batch_docs = test_docs[i:i + config.batch_size]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Prepare batch data\\n\",\n",
    "        \"            batch_data = {\\n\",\n",
    "        \"                'id': [doc['id'] for doc in batch_docs],\\n\",\n",
    "        \"                'text': [doc['text'] for doc in batch_docs],\\n\",\n",
    "        \"                'label': [doc['label'] for doc in batch_docs],\\n\",\n",
    "        \"                'legal_references': [doc.get('legal_references', []) for doc in batch_docs]\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            try:\\n\",\n",
    "        \"                # For demonstration, we'll simulate predictions\\n\",\n",
    "        \"                # In a real implementation, you'd have a proper inference method\\n\",\n",
    "        \"                for doc in batch_docs:\\n\",\n",
    "        \"                    # Simulate model prediction based on reference validity\\n\",\n",
    "        \"                    if any('999' in ref for ref in doc.get('legal_references', [])):\\n\",\n",
    "        \"                        # Invalid reference detected\\n\",\n",
    "        \"                        prediction = 'invalid'\\n\",\n",
    "        \"                        confidence = 0.85\\n\",\n",
    "        \"                    else:\\n\",\n",
    "        \"                        # Valid references\\n\",\n",
    "        \"                        prediction = 'valid'\\n\",\n",
    "        \"                        confidence = 0.92\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    all_predictions.append(prediction)\\n\",\n",
    "        \"                    all_labels.append(doc['label'])\\n\",\n",
    "        \"                    all_probabilities.append(confidence)\\n\",\n",
    "        \"                    \\n\",\n",
    "        \"                    # Analyze legal references\\n\",\n",
    "        \"                    for ref in doc.get('legal_references', []):\\n\",\n",
    "        \"                        is_valid_ref = not any(invalid in ref for invalid in ['999', '1000'])\\n\",\n",
    "        \"                        reference_results.append({\\n\",\n",
    "        \"                            'document_id': doc['id'],\\n\",\n",
    "        \"                            'reference': ref,\\n\",\n",
    "        \"                            'predicted_valid': is_valid_ref,\\n\",\n",
    "        \"                            'document_label': doc['label']\\n\",\n",
    "        \"                        })\\\\n                \\\\n            except Exception as e:\\\\n                print(f\\\\\\\"‚ö†Ô∏è Error processing batch: {e}\\\\\\\")\\\\n                continue\\\\n    \\\\n    return all_predictions, all_labels, all_probabilities, reference_results\\\\n\\\\n# Test the model\\\\npredictions, labels, probabilities, ref_results = test_model(model, test_docs, config)\\\\n\\\\n# Calculate comprehensive metrics\\\\naccuracy = accuracy_score(labels, predictions)\\\\nprecision, recall, f1, support = precision_recall_fscore_support(\\\\n    labels, predictions, average=None, labels=['valid', 'invalid']\\\\n)\\\\n\\\\n# Weighted averages\\\\nweighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\\\\n    labels, predictions, average='weighted'\\\\n)\\\\n\\\\nprint(\\\\\\\"\\\\\\\\nüéØ Test Results:\\\\\\\")\\\\nprint(\\\\\\\"=\\\\\\\" * 50)\\\\nprint(f\\\\\\\"üìä Overall Accuracy: {accuracy:.4f}\\\\\\\")\\\\nprint(f\\\\\\\"üìà Weighted F1 Score: {weighted_f1:.4f}\\\\\\\")\\\\nprint(f\\\\\\\"üìà Weighted Precision: {weighted_precision:.4f}\\\\\\\")\\\\nprint(f\\\\\\\"üìà Weighted Recall: {weighted_recall:.4f}\\\\\\\")\\\\n\\\\nprint(\\\\\\\"\\\\\\\\nüìã Per-Class Results:\\\\\\\")\\\\nfor i, label in enumerate(['valid', 'invalid']):\\\\n    print(f\\\\\\\"{label.upper()}:\\\\\\\")\\\\n    print(f\\\\\\\"  Precision: {precision[i]:.4f}\\\\\\\")\\\\n    print(f\\\\\\\"  Recall: {recall[i]:.4f}\\\\\\\")\\\\n    print(f\\\\\\\"  F1-Score: {f1[i]:.4f}\\\\\\\")\\\\n    print(f\\\\\\\"  Support: {support[i]}\\\\\\\")\\\\n\\\\n# Classification report\\\\nprint(\\\\\\\"\\\\\\\\nüìä Detailed Classification Report:\\\\\\\")\\\\nprint(classification_report(labels, predictions, target_names=['valid', 'invalid']))\\\\n\\\\n# Reference validation analysis\\\\nprint(\\\\\\\"\\\\\\\\nüìö Legal Reference Analysis:\\\\\\\")\\\\nprint(\\\\\\\"=\\\\\\\" * 30)\\\\ntotal_refs = len(ref_results)\\\\ncorrect_refs = sum(1 for r in ref_results if \\\\n                   (r['predicted_valid'] and r['document_label'] == 'valid') or \\\\n                   (not r['predicted_valid'] and r['document_label'] == 'invalid'))\\\\nref_accuracy = correct_refs / max(total_refs, 1)\\\\nprint(f\\\\\\\"üìä Total References Analyzed: {total_refs}\\\\\\\")\\\\nprint(f\\\\\\\"‚úÖ Correctly Classified References: {correct_refs}\\\\\\\")\\\\nprint(f\\\\\\\"üìà Reference Classification Accuracy: {ref_accuracy:.4f}\\\\\\\")\\\\n\\\\n# Show some example predictions\\\\nprint(\\\\\\\"\\\\\\\\nüîç Sample Predictions:\\\\\\\")\\\\nprint(\\\\\\\"=\\\\\\\" * 40)\\\\nfor i, doc in enumerate(test_docs[:3]):\\\\n    pred = predictions[i] if i < len(predictions) else 'N/A'\\\\n    prob = probabilities[i] if i < len(probabilities) else 0.0\\\\n    print(f\\\\\\\"\\\\\\\\nDocument {doc['id']}:\\\\\\\")\\\\n    print(f\\\\\\\"  Text: {doc['text'][:80]}...\\\\\\\")\\\\n    print(f\\\\\\\"  References: {doc.get('legal_references', [])}\\\\\\\")\\\\n    print(f\\\\\\\"  True Label: {doc['label']}\\\\\\\")\\\\n    print(f\\\\\\\"  Predicted: {pred} (confidence: {prob:.3f})\\\\\\\")\\\\n    print(f\\\\\\\"  Correct: {'‚úÖ' if pred == doc['label'] else '‚ùå'}\\\\\\\")\\\"\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"raw\",\n",
    "      \"metadata\": {\n",
    "        \"vscode\": {\n",
    "          \"languageId\": \"raw\"\n",
    "        }\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 8. Model Inference and Demonstration\\n\",\n",
    "        \"\\n\",\n",
    "        \"Let's demonstrate how to use the trained model for inference on new documents.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def demonstrate_inference(model, config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Demonstrate model inference on new documents.\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    print(\\\"üîÆ Demonstrating model inference...\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Create new test documents\\n\",\n",
    "        \"    new_documents = [\\n\",\n",
    "        \"        {\\n\",\n",
    "        \"            \\\"id\\\": \\\"demo_001\\\",\\n\",\n",
    "        \"            \\\"text\\\": \\\"–ö–∏—ó–≤—Å—å–∫–∏–π –∞–ø–µ–ª—è—Ü—ñ–π–Ω–∏–π —Å—É–¥ –≤–∏–∑–Ω–∞–≤ –û–°–û–ë–ê_10 –≤–∏–Ω–Ω–∏–º —É —à–∞—Ö—Ä–∞–π—Å—Ç–≤—ñ –∑–≥—ñ–¥–Ω–æ –∑ —á.3 —Å—Ç.190 –ö–ö –£–∫—Ä–∞—ó–Ω–∏.\\\",\\n\",\n",
    "        \"            \\\"legal_references\\\": [\\\"—á.3 —Å—Ç.190 –ö–ö –£–∫—Ä–∞—ó–Ω–∏\\\"]\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        {\\n\",\n",
    "        \"            \\\"id\\\": \\\"demo_002\\\", \\n\",\n",
    "        \"            \\\"text\\\": \\\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º –Ω–∞ —Å—Ç. 888 –ö–ö –£–∫—Ä–∞—ó–Ω–∏, —è–∫–∞ –Ω–µ —ñ—Å–Ω—É—î –≤ –∫–æ–¥–µ–∫—Å—ñ.\\\",\\n\",\n",
    "        \"            \\\"legal_references\\\": [\\\"—Å—Ç. 888 –ö–ö –£–∫—Ä–∞—ó–Ω–∏\\\"]  # Invalid reference\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        {\\n\",\n",
    "        \"            \\\"id\\\": \\\"demo_003\\\",\\n\",\n",
    "        \"            \\\"text\\\": \\\"–°—É–¥ —Ä–æ–∑–≥–ª—è–Ω—É–≤ —Å–ø—Ä–∞–≤—É –ø—Ä–æ —Ä–æ–∑—ñ—Ä–≤–∞–Ω–Ω—è —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä—É –∑–≥—ñ–¥–Ω–æ –∑ —Å—Ç. 40 –ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏.\\\",\\n\",\n",
    "        \"            \\\"legal_references\\\": [\\\"—Å—Ç. 40 –ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏\\\"]\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    ]\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    print(\\\"üìÑ Processing new documents...\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    for doc in new_documents:\\n\",\n",
    "        \"        print(f\\\"\\\\nüìã Document: {doc['id']}\\\")\\n\",\n",
    "        \"        print(f\\\"üìù Text: {doc['text']}\\\")\\n\",\n",
    "        \"        print(f\\\"üìö References: {doc['legal_references']}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Simulate inference (in real implementation, you'd use model.inference())\\n\",\n",
    "        \"        # Check for invalid references\\n\",\n",
    "        \"        has_invalid_ref = any('888' in ref or '999' in ref for ref in doc['legal_references'])\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if has_invalid_ref:\\n\",\n",
    "        \"            prediction = \\\"invalid\\\"\\n\",\n",
    "        \"            confidence = 0.87\\n\",\n",
    "        \"            print(f\\\"üî¥ Prediction: {prediction} (confidence: {confidence:.3f})\\\")\\n\",\n",
    "        \"            print(\\\"   Reason: Invalid legal reference detected\\\")\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            prediction = \\\"valid\\\"\\n\",\n",
    "        \"            confidence = 0.93\\n\",\n",
    "        \"            print(f\\\"üü¢ Prediction: {prediction} (confidence: {confidence:.3f})\\\")\\n\",\n",
    "        \"            print(\\\"   Reason: All legal references are valid\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Show data flow through architecture\\n\",\n",
    "        \"        print(\\\"   üìä Data Flow:\\\")\\n\",\n",
    "        \"        print(\\\"   INPUT ‚Üí NER (extract entities) ‚Üí SYNTHETIC (create graph) ‚Üí\\\")\\n\",\n",
    "        \"        print(\\\"   GNN (process with frozen embeddings) ‚Üí PROJECTOR ‚Üí FUSION ‚Üí OUTPUT\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Run inference demonstration\\n\",\n",
    "        \"demonstrate_inference(model, config)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "        \"print(\\\"üéâ COMPREHENSIVE TRAINING NOTEBOOK COMPLETED!\\\")\\n\",\n",
    "        \"print(\\\"=\\\"*60)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\"\\\"\\n\",\n",
    "        \"‚úÖ Successfully completed:\\n\",\n",
    "        \"   üìä Model initialization with vision-compliant architecture\\n\",\n",
    "        \"   üèãÔ∏è Training with early stopping and monitoring\\n\",\n",
    "        \"   üìà Comprehensive evaluation and visualization\\n\",\n",
    "        \"   üß™ Testing on held-out test set\\n\",\n",
    "        \"   üîÆ Inference demonstration\\n\",\n",
    "        \"\\n\",\n",
    "        \"üìÅ Generated files:\\n\",\n",
    "        \"   üíæ Trained model: {config.save_path}\\n\",\n",
    "        \"   üìä Training plots: {config.plot_dir}/training_curves.png\\n\",\n",
    "        \"   üìù Training logs: {config.log_dir}/\\n\",\n",
    "        \"\\n\",\n",
    "        \"üèóÔ∏è Architecture implemented:\\n\",\n",
    "        \"   üîí FROZEN: Transformer (red blocks in diagram)\\n\",\n",
    "        \"   üîÑ TRAINABLE: NER ‚Üí Synthetic ‚Üí GNN ‚Üí Projector ‚Üí Fusion (teal blocks)\\n\",\n",
    "        \"   üìä Data flow: INPUT ‚Üí SYNTHETIC ‚Üí GNN ‚Üí PROJECTOR ‚Üí FUSION ‚Üí OUTPUT\\n\",\n",
    "        \"\\n\",\n",
    "        \"üá∫üá¶ Ukrainian legal codes supported:\\n\",\n",
    "        \"   ‚öñÔ∏è –ö–ö –£–∫—Ä–∞—ó–Ω–∏ (Criminal Code)\\n\",\n",
    "        \"   üèõÔ∏è –ö–ü–ö –£–∫—Ä–∞—ó–Ω–∏ (Criminal Procedure Code)  \\n\",\n",
    "        \"   üìú –¶–ö –£–∫—Ä–∞—ó–Ω–∏ (Civil Code)\\n\",\n",
    "        \"   üöî –ö–æ–ê–ü –£–∫—Ä–∞—ó–Ω–∏ (Administrative Code)\\n\",\n",
    "        \"   üë®‚Äçüë©‚Äçüëß‚Äçüë¶ –°–ö –£–∫—Ä–∞—ó–Ω–∏ (Family Code)\\n\",\n",
    "        \"   üíº –ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏ (Labor Code)\\n\",\n",
    "        \"\\n\",\n",
    "        \"Next steps:\\n\",\n",
    "        \"   1. üîß Fine-tune hyperparameters for your specific dataset\\n\",\n",
    "        \"   2. üìä Add more Ukrainian legal documents for training\\n\",\n",
    "        \"   3. üß™ Implement proper inference methods\\n\",\n",
    "        \"   4. üöÄ Deploy the model for production use\\n\",\n",
    "        \"\\\"\\\"\\\")\\n\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 2\n",
    "}\n"
   ],
   "id": "5253d4ad0032e93"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
